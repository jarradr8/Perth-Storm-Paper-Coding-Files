{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6f5e13-1151-481d-b92c-98a4571dab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from bisect import bisect_left\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import (norm, skew, kurtosis, pearsonr, spearmanr, kendalltau)\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pysal\n",
    "from numpy.random import Generator, PCG64\n",
    "from functools import partial\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "#!pip install docx\n",
    "from docx import Document\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7139fb-8acb-44d1-aca6-99a46a0608ca",
   "metadata": {},
   "source": [
    "# Fix 'from docx import Document' import by removing the wrong package and installing the right one\n",
    "\n",
    "import sys, os, pathlib, importlib.util, subprocess\n",
    "\n",
    "def run(cmd):\n",
    "    print(f\"\\n=== $ {cmd}\")\n",
    "    return subprocess.call(cmd, shell=True)\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Active environment site-packages paths (top 5):\", sys.path[:5])\n",
    "\n",
    "# 1) Uninstall the wrong package ('docx') if present\n",
    "run(\"pip uninstall -y docx\")\n",
    "\n",
    "# 2) Install the correct package ('python-docx')\n",
    "# Prefer conda if you're using Anaconda; fall back to pip if conda isn't available.\n",
    "if shutil := importlib.util.find_spec(\"shutil\"):\n",
    "    import shutil as _sh\n",
    "    if _sh.which(\"conda\"):\n",
    "        run(\"conda install -y -c conda-forge python-docx\")\n",
    "    else:\n",
    "        run(\"pip install -U python-docx\")\n",
    "else:\n",
    "    # very defensive fallback\n",
    "    run(\"pip install -U python-docx\")\n",
    "\n",
    "# 3) Guard against a local file shadowing the package (e.g., a file named 'docx.py' in your folder)\n",
    "cwd_files = {p.name for p in pathlib.Path('.').iterdir() if p.is_file()}\n",
    "if \"docx.py\" in cwd_files or \"docx\" in cwd_files:\n",
    "    print(\"\\n⚠️ Detected a local file named 'docx.py' or 'docx' in your working directory.\")\n",
    "    print(\"   This will shadow the 'python-docx' package. Rename that file and re-run this cell.\")\n",
    "\n",
    "# 4) Test the import\n",
    "print(\"\\n=== Import test ===\")\n",
    "try:\n",
    "    from docx import Document\n",
    "    print(\"✅ Import succeeded. Version check below:\")\n",
    "    import docx\n",
    "    print(\"python-docx version:\", getattr(docx, \"__version__\", \"unknown\"))\n",
    "except Exception as e:\n",
    "    print(\"❌ Import still failing:\", repr(e))\n",
    "    print(\"If this persists, restart the kernel (Kernel → Restart) and run this cell again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bf7462-c30f-4ff4-8caf-dbd5ad471faa",
   "metadata": {},
   "source": [
    "# CREATE A FUNCTION THAT SLICES THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7931fbf7-f2ac-47f1-a1e4-ab5169f3d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe_by_months(dataframe, months):\n",
    "    \"\"\"\n",
    "    Filters rows from a DataFrame based on the month of its DateTime index.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The DataFrame to filter. Must have a DateTime index.\n",
    "    - months (list of int): List of months to filter for (e.g., [1, 2, 3] for Jan, Feb, Mar).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing only the rows for the specified months.\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if not isinstance(months, list) or not all(isinstance(month, int) for month in months):\n",
    "        raise ValueError(\"Months must be provided as a list of integers.\")\n",
    "    if not hasattr(dataframe.index, \"month\"):\n",
    "        raise ValueError(\"The DataFrame must have a DateTime index.\")\n",
    "\n",
    "    # Perform filtering\n",
    "    return dataframe[dataframe.index.month.isin(months)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c175d18-fcbe-4147-95c6-74e6e11f94da",
   "metadata": {},
   "source": [
    "# rolling cross correlate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c9e6eb6-c47a-4379-8c90-b6d4f6461702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_cross_correlate(df1, df2, col1, col2, lag=-3, window=366):\n",
    "    \"\"\"\n",
    "    Computes rolling cross-correlation between two time-series columns from \n",
    "    two datetime-indexed DataFrames with a specific lag.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : pd.DataFrame\n",
    "        First DataFrame with a datetime index.\n",
    "    df2 : pd.DataFrame\n",
    "        Second DataFrame with a datetime index.\n",
    "    col1 : str\n",
    "        Column name in the first DataFrame to use for correlation.\n",
    "    col2 : str\n",
    "        Column name in the second DataFrame to use for correlation.\n",
    "    lag : int, optional\n",
    "        Lag (in time steps) to consider. Default is -3.\n",
    "    window : int, optional\n",
    "        Rolling window size (in days) for the correlation. Default is 366.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    rolling_correlations : pd.Series\n",
    "        Rolling cross-correlation values for the specified lag.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df1 = df1.dropna()\n",
    "    df2 = df2.dropna()\n",
    "    \n",
    "    # Align data based on datetime index\n",
    "    merged = pd.merge(df1[[col1]], df2[[col2]], left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    # Shift the second series by the specified lag\n",
    "    shifted_series2 = merged[col2].shift(-lag)\n",
    "    \n",
    "    # Calculate rolling correlation\n",
    "    rolling_correlations, p_value = merged[[col1, col2]].rolling(window=window).corr(shifted_series2).iloc[:, 0]\n",
    "\n",
    "    return rolling_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b2bb3-c756-48fe-94ab-b16681ba37ad",
   "metadata": {},
   "source": [
    "# 2. ADJUST ZAK BAILLIES SIGNIFICANT TEST P-VALE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78bbbbac-8902-4bac-b104-d1a191071fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the correlation value based on a Pearsons principle and also checks whether the significance of the relationship is true on a \n",
    "#95% and 99% level\n",
    "def corr_sig(x, y):\n",
    "    \"\"\"\n",
    "    Calculate Pearson correlation and test if p-value <= 0.05.\n",
    "    \n",
    "    Returns:\n",
    "        corr (float): Correlation coefficient\n",
    "        is_significant (bool): True if p <= 0.01, else False, whcih is the 99th percentile\n",
    "    \"\"\"\n",
    "    corr, p_value = pearsonr(x, y)\n",
    "    if p_value < 0.01:\n",
    "        significance = '99%'  # 99% confidence\n",
    "    elif p_value < 0.05:\n",
    "        significance = '95%'   # 95% confidence\n",
    "    else:\n",
    "        significance = ''\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Correlation': corr.round(2),\n",
    "        'p_value': p_value.round(2),\n",
    "        'Significance': significance})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e8d4e0-f31f-42d1-9e25-f34b0bd804a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "3 // 2 # gives the window to the lowest value - half a window is 1+5x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547be701-7aa7-4783-b114-8155b7cf3b40",
   "metadata": {},
   "source": [
    "# 10. Function For Rolling Correlation On a 366 Day Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2952a571-415c-4a23-8d52-cf03e145fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_correlation_with_p(df, col1, col2, window, min_valid_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Calculate running centered correlation and significance between two columns.\n",
    "    NaN if any value missing in the window or if too much data is missing.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with DateTime index\n",
    "        col1: First column name\n",
    "        col2: Second column name\n",
    "        window: Window size (must be odd)\n",
    "        min_valid_ratio: Minimum proportion of valid data required to calculate correlation\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame with 'correlation' and 'significant' columns\n",
    "    \"\"\"\n",
    "    if window % 2 == 0:\n",
    "        raise ValueError(\"Window size must be odd for centered correlation.\")\n",
    "    \n",
    "    half_window = window // 2 # gives the window to the lowest value - half a window is 1+5x2\n",
    "    corr_values = []\n",
    "    sig_values = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i - half_window < 0 or i + half_window >= len(df):\n",
    "            corr_values.append(np.nan)\n",
    "            sig_values.append(np.nan)\n",
    "            continue\n",
    "        sub = df.iloc[i - half_window : i + half_window + 1][[col1, col2]]\n",
    "        valid = sub.dropna()\n",
    "        if len(valid) / window < min_valid_ratio:  #This checks to see if avalaible data that can be used, removes data with less then 70%\n",
    "            # Too much missing data\n",
    "            corr_values.append(np.nan)\n",
    "            sig_values.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        corr, p_value = pearsonr(valid[col1], valid[col2])\n",
    "        corr_values.append(corr)\n",
    "        significance = (\n",
    "            '95%' if p_value < 0.05 else\n",
    "            '90%' if p_value < 0.1 else\n",
    "            '')\n",
    "        \n",
    "        sig_values.append(significance)\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'correlation': corr_values,\n",
    "        'significant': sig_values\n",
    "    }, index=df.index)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d695d-c54a-44a5-a3be-2958ae170616",
   "metadata": {},
   "source": [
    "# WMO GUIDELINES MONTHLY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6d969d-84d6-44cc-a6cf-b361c687f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMO Guidelined Months\n",
    "def WMO_Guide_Months(D,Date_Column,Type_Column,Raindays):    \n",
    "\n",
    "    #The appendenation of the daily data into monthlies\n",
    "    Usuable = [] #WMO Standard \n",
    "    #Since we have dropped all periods with missing days lets work out days avalaible\n",
    "    count_for_month = D.resample('MS').count().reset_index()\n",
    "\n",
    "    #Fill in all days where no data is avalaible with nans\n",
    "    nan_dates = pd.date_range(start=D.index.min(), end=D.index.max(), freq='D') \n",
    "    # Create an empty DataFrame with these monthly dates as index and label it MSLP\n",
    "    Data_Missing = pd.DataFrame(index=nan_dates, columns=[Type_Column])\n",
    "    Data_Missing.index.name = Date_Column  # Set the index name to 'Date'\n",
    "    Data_Missing[:] = np.nan\n",
    "    D = pd.concat([D, Data_Missing[~Data_Missing.index.isin(D.index)]])\n",
    "    D = D.sort_index()\n",
    "\n",
    "    #Get it into monthlies\n",
    "    if (Raindays == True):\n",
    "        M = D.resample('MS').sum()\n",
    "    else:\n",
    "        M = D.resample('MS').mean()\n",
    "\n",
    "    #Reset Index - easier to apply on the for loop\n",
    "    M = M.reset_index()\n",
    "\n",
    "    #Initial check is to remove monthlies where 11 days or more are missing\n",
    "    for i in range(0, len(count_for_month)):\n",
    "        #Get the year and month \n",
    "        Year = count_for_month[Date_Column].loc[i].year\n",
    "        Month = count_for_month[Date_Column].loc[i].month\n",
    "\n",
    "        #Check for leap year and total days\n",
    "        Total_Monthly_Count = [31,29 if Year % 4 == 0 else 28,31,30,31,30,31,31,30,31,30,31]\n",
    "\n",
    "        #Chekc the number of days avalaible for that month\n",
    "        Count_Month_Total = Total_Monthly_Count[Month-1]\n",
    "        \n",
    "        #Check the count\n",
    "        Count = count_for_month.loc[i].loc[Type_Column] #Type column is Rainfall/Raindays/MLSP\n",
    "    \n",
    "        #If the count is less then 11, then we keep it\n",
    "        if Count < Count_Month_Total - 11: #11 randomly missing days\n",
    "            x = 0\n",
    "        else:\n",
    "            #Now we check whether it has 5 or more consecutive days missing\n",
    "            #Checks current count\n",
    "            Consec_NaNs = 0\n",
    "            #Checks max count of the month\n",
    "            Consec_Max = 0\n",
    "                \n",
    "            Months_Data = D.loc[\"{}-{}\".format(Year, Month)].reset_index() #Extract the months daily data\n",
    "            \n",
    "            for q in range(0, len(Months_Data)):\n",
    "                #If daily data is a nan, it will add to Consec NaNs\n",
    "                if np.isnan(Months_Data[Type_Column].loc[q]) == True:\n",
    "                    Consec_NaNs = Consec_NaNs + 1\n",
    "                else:\n",
    "                    #if not a nan, it will save the nan check count\n",
    "                    Consec_Check = Consec_NaNs\n",
    "                    #The save check count is then checked to see if its > then the max of that month\n",
    "                    if Consec_Check >= Consec_Max:\n",
    "                        #If so then the max check count is then updated\n",
    "                        Consec_Max = Consec_Check\n",
    "                    Consec_NaNs = 0\n",
    "\n",
    "            #Once the month has been checked - if the max check count is 5 or more, then its a nan\n",
    "            if Consec_Max >= 5: # this chekcs to on more then 5 consecutive days\n",
    "               x = 0\n",
    "            else: \n",
    "                Usuable.append(M.loc[i])\n",
    "    \n",
    "    #####################\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    Usuable = pd.DataFrame(Usuable)\n",
    "    \n",
    "    # Convert 'Date' column to datetime\n",
    "    Usuable[Date_Column] = pd.to_datetime(Usuable[Date_Column])\n",
    "    \n",
    "    # Set 'Date' column as index\n",
    "    Usuable.set_index(Date_Column, inplace=True)\n",
    "\n",
    "    #Ensure all monthly dates are filled with NaNs\n",
    "    start_date = Usuable.index.min()\n",
    "    end_date = Usuable.index.max()\n",
    "    # Create a monthly date range\n",
    "    monthly_dates = pd.date_range(start=start_date, end=end_date, freq='MS')  # 'MS' = Month Start\n",
    "    # Create an empty DataFrame with these monthly dates as index and label it MSLP\n",
    "    MSLP_Missing = pd.DataFrame(index=monthly_dates, columns=[Type_Column])\n",
    "    MSLP_Missing.index.name = Date_Column  # Set the index name to 'Date'\n",
    "    MSLP_Missing[:] = np.nan\n",
    "    \n",
    "    Usuable_infil = pd.concat([Usuable, MSLP_Missing[~MSLP_Missing.index.isin(Usuable.index)]])\n",
    "    Usuable_infil = Usuable_infil.sort_index()\n",
    "    \n",
    "    return(Usuable_infil.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a44449-2748-4b4f-97e8-bd0f5ca476da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WMO Guidelined Months\n",
    "def WMO_Guide_Yearly(M,Date_Column,Type_Column,Raindays):\n",
    "    Usuable = [] #WMO Standard \n",
    "    #The appendenation of the daily data into monthlies\n",
    "    Usuable = [] #WMO Standard \n",
    "    #Since we have dropped all periods with missing days lets work out days avalaible\n",
    "    count_for_year = M.resample('YS').count().reset_index()\n",
    "\n",
    "    #Fill in all days where no data is avalaible with nans\n",
    "    nan_dates = pd.date_range(start=M.index.min(), end=M.index.max(), freq='MS') \n",
    "    # Create an empty DataFrame with these monthly dates as index and label it MSLP\n",
    "    Data_Missing = pd.DataFrame(index=nan_dates, columns=[Type_Column])\n",
    "    Data_Missing.index.name = Date_Column  # Set the index name to 'Date'\n",
    "    Data_Missing[:] = np.nan\n",
    "    M = pd.concat([M, Data_Missing[~Data_Missing.index.isin(M.index)]])\n",
    "    M = M.sort_index()\n",
    "\n",
    "    #Get it into monthlies\n",
    "    if (Raindays == True):\n",
    "        Y = M.resample('YS').sum()\n",
    "    else:\n",
    "        Y = M.resample('YS').mean()\n",
    "\n",
    "    #Reset Index - easier to apply on the for loop\n",
    "    Y = Y.reset_index()\n",
    "\n",
    "    #Initial check is to remove monthlies where 11 days or more are missing\n",
    "    for i in range(0, len(count_for_year)):\n",
    "        #Get the year and month \n",
    "        Year = count_for_year[Date_Column].loc[i].year\n",
    "\n",
    "        #Check for leap year and total days\n",
    "        Total_Yearly_Count = 12\n",
    "        \n",
    "        #Chekc the number of days avalaible for that month\n",
    "        Count_Yearly_Total = Total_Yearly_Count\n",
    "        \n",
    "        #Check the count\n",
    "        Count = count_for_year.loc[i].loc[Type_Column] #Type column is Rainfall/Raindays/MLSP\n",
    "    \n",
    "        #If the count is less then 11, then we keep it\n",
    "        if Count < Count_Yearly_Total - 5: #5 randomly missing months\n",
    "            x = 0\n",
    "        else:\n",
    "            #Now we check whether it has 5 or more consecutive days missing\n",
    "            #Checks current count\n",
    "            Consec_NaNs = 0\n",
    "            #Checks max count of the month\n",
    "            Consec_Max = 0\n",
    "                \n",
    "            Yearly_Data = M.loc[\"{}\".format(Year)].reset_index() #Extract the months daily data\n",
    "            \n",
    "            for q in range(0, len(Yearly_Data)):\n",
    "                #If daily data is a nan, it will add to Consec NaNs\n",
    "                if np.isnan(Yearly_Data[Type_Column].loc[q]) == True:\n",
    "                    Consec_NaNs = Consec_NaNs + 1\n",
    "                else:\n",
    "                    #if not a nan, it will save the nan check count\n",
    "                    Consec_Check = Consec_NaNs\n",
    "                    #The save check count is then checked to see if its > then the max of that month\n",
    "                    if Consec_Check >= Consec_Max:\n",
    "                        #If so then the max check count is then updated\n",
    "                        Consec_Max = Consec_Check\n",
    "                    Consec_NaNs = 0\n",
    "\n",
    "            #Once the month has been checked - if the max check count is 5 or more, then its a nan\n",
    "            if Consec_Max >= 3: # this chekcs to on more then 5 consecutive months\n",
    "               x = 0\n",
    "            else: \n",
    "                Usuable.append(Y.loc[i])\n",
    "    \n",
    "    #####################\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    Usuable = pd.DataFrame(Usuable)\n",
    "    \n",
    "    # Convert 'Date' column to datetime\n",
    "    Usuable[Date_Column] = pd.to_datetime(Usuable[Date_Column])\n",
    "    \n",
    "    # Set 'Date' column as index\n",
    "    Usuable.set_index(Date_Column, inplace=True)\n",
    "\n",
    "    #Ensure all monthly dates are filled with NaNs\n",
    "    start_date = Usuable.index.min()\n",
    "    end_date = Usuable.index.max()\n",
    "    # Create a monthly date range\n",
    "    yearly_dates = pd.date_range(start=start_date, end=end_date, freq='YS')  # 'MS' = Month Start\n",
    "    # Create an empty DataFrame with these monthly dates as index and label it MSLP\n",
    "    MSLP_Missing = pd.DataFrame(index=yearly_dates, columns=[Type_Column])\n",
    "    MSLP_Missing.index.name = Date_Column  # Set the index name to 'Date'\n",
    "    MSLP_Missing[:] = np.nan\n",
    "    \n",
    "    Usuable_infil = pd.concat([Usuable, MSLP_Missing[~MSLP_Missing.index.isin(Usuable.index)]])\n",
    "    Usuable_infil = Usuable_infil.sort_index()\n",
    "    \n",
    "    return(Usuable_infil.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba126f16-22ad-48b1-9b53-b221c0dc4975",
   "metadata": {},
   "source": [
    "# SUBDAILY TO YEARLY INCORPORATING WMO GUIDELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d7c19b6-8fae-4a6f-ba6c-4903c84006d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SD_Y_WMO_GL(Data,Date_Col,Type_Col,Rain):\n",
    "    if(Rain==True):\n",
    "        #Raindays/Rainfall means no sub-dailies\n",
    "        Data_D = Data.dropna() #Exclude all NaN data\n",
    "        #Monthly data needs to be subjected to WMO guidelines\n",
    "        Data_M = WMO_Guide_Months(Data_D,Date_Col,Type_Col,True)\n",
    "        #Now apply similar guideline ratios onto the Yearly\n",
    "        Data_Y = WMO_Guide_Yearly(Data_M.dropna(),Date_Col,Type_Col,True)\n",
    "        return(Data_D,Data_M,Data_Y)\n",
    "    else:\n",
    "        #Pressure cna be averaged\n",
    "        Data_SD = Data.dropna() #Exclude all NaN data\n",
    "        Data_D = Data_SD.resample('D').mean()\n",
    "        #Monthly data needs to be subjected to WMO guidelines\n",
    "        Data_M = WMO_Guide_Months(Data_D,Date_Col,Type_Col,False)\n",
    "        #Now apply similar guideline ratios onto the Yearly\n",
    "        Data_Y = WMO_Guide_Yearly(Data_M.dropna(),Date_Col,Type_Col,False)\n",
    "        return(Data_SD,Data_D,Data_M,Data_Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e65ed-be9d-48e4-ac8a-946a91986291",
   "metadata": {},
   "source": [
    "# UQC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c4313fe-c1ca-40a8-9d76-f6fd9d9a2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zaks UQC format scripts turned into a single fucntion\n",
    "def ymd_column(df):\n",
    "    \"\"\"adds a year month and day column to a given dataframe\n",
    "    \n",
    "    args: df containing datetime index\n",
    "    \"\"\"\n",
    "    \n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-3:] + cols[:-3]\n",
    "    df = df[cols]\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "def UQC_pres_formats(df):\n",
    "    data = df\n",
    "    app_data = []\n",
    "    for value in sorted(data.index.hour.unique().values):\n",
    "        subset = data[data.index.hour == value]\n",
    "        subset.index = subset.index.normalize()\n",
    "        subset.columns = ['SLP{}'.format(str(value))]\n",
    "        #Added in cause there was a duplicate somewhere\n",
    "        subset = subset[~subset.index.duplicated(keep='first')]\n",
    "        app_data.append(subset)\n",
    "    \n",
    "    UQC_formats = pd.concat(app_data,axis=1)   \n",
    "\n",
    "    UQC_formats = ymd_column(UQC_formats)\n",
    "    UQC_formats = UQC_formats.round(4)\n",
    "    UQC_formats.fillna(-99.9,inplace=True)\n",
    "    return UQC_formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d51243-a945-4c51-b264-86bece2b8ec0",
   "metadata": {},
   "source": [
    "# FARHEINHEIGHT TO CELSUIS AND VICE VERSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae120a4-9f00-412a-83cf-68bff4bff3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_to_c(temp):\n",
    "    \"\"\"Convert Fahrenheit to Celsius, ignore -99.9 (missing value).\"\"\"\n",
    "    return round((temp - 32) * 5/9, 1)\n",
    "\n",
    "def c_to_f(temp):\n",
    "    \"\"\"Convert Fahrenheit to Celsius, ignore -99.9 (missing value).\"\"\"\n",
    "    return round((temp*(9/5)) + 32, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd50291c-b7b7-48d7-bca5-b5e1bc489342",
   "metadata": {},
   "source": [
    "# AS READ TO MSLP FUNCTIONS : \n",
    "## CORRECT TEMPERATURE\n",
    "## GRAVITY CALCULATOR\n",
    "## CORRECT GRAVITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72efba77-016c-4df6-ad93-a621546c49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall function version 2\n",
    "def Reduce_to_MSLP_V2_2(Pressure_Data, Temperature_Data, \n",
    "                      Barometer_Type, Pressure_Unit, \n",
    "                      Temperature_Unit, latitude, \n",
    "                      elevation_feet, elevation_type):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    -------------------\n",
    "    \n",
    "    Pressure_Data : DATAFRAME\n",
    "        Ensure the date is in the index column, pressure can be any of the typical units used, and make sure it\n",
    "        has a label\n",
    "        \n",
    "    Temperature_Data : DATAFRAME\n",
    "        Ensure the date is in the index column, temperature can be any of the typical units used, and make sure it\n",
    "        has a label\n",
    "    \n",
    "    Barometer_Type : STRING\n",
    "        What is the type of barometer used for this case. If you do not know use \"Unknown\", but the other barometer \n",
    "        types are \"Fortin\", \"Cistern\", \"Fuess\" and \"Kew\"\n",
    "        \n",
    "    Pressure_Unit : STRING\n",
    "        What is the pressure units you use. It is \"inHg\", \"mmHg\", \"mb\", \"bar\", \"hPa\", 'Pa', or 'atm'\n",
    "    \n",
    "    Temperature_Unit : STRING\n",
    "        What is the temperature units you use. It is \"degF\", \"degC\", or \"K\"\n",
    "        \n",
    "    latitude : RATIONAL NUMBER\n",
    "        Where is the location of the barometer to best possible coordinate value you can find.\n",
    "    \n",
    "    elevation_feet : RATIONAL NUMBER (ft)\n",
    "        What is the elevation of the barometer to the best possible knowledge in feet (ft).\n",
    "        \n",
    "    elevation_type : STRING\n",
    "        What is the units of the elevation, is it meters/metres, m, ft or feet\n",
    "        \n",
    "    OUTPUTS:\n",
    "    ---------------------\n",
    "    \n",
    "    Final_Product : DATAFRAME\n",
    "        So this has all the necassary values such as MSLP in hPa which is the corrected to gravity and temperature,\n",
    "        the temperature in degC, and the original pressure and adjusted to temperature pressure in the columns\n",
    "        in that respective order.\n",
    "    '''\n",
    "    #Convert up the data so Pressure is in mb and temperature is in degC\n",
    "    PT_Fixed = Converters(Pressure_Data,Temperature_Data,Pressure_Unit,Temperature_Unit)  \n",
    "    if PT_Fixed == 'Error Not a Viable Pressure Unit':\n",
    "        return('Error Not a Viable Pressure Unit')\n",
    "    elif PT_Fixed == 'Error Not a Viable Temperature Unit':\n",
    "        return('Error Not a Viable Temperature Unit')\n",
    "    else:\n",
    "        X = 0\n",
    "    #Caculate the Barometer reading freduced to standard temperature but not to standard gravity\n",
    "    Pres_T = Correct_Temp(PT_Fixed[0],PT_Fixed[1],Barometer_Type)  \n",
    "\n",
    "    #Calculate GOH, this is local gravity for the barometer\n",
    "    GOH,Elevation = calculate_gravity(latitude, elevation_feet,elevation_type)\n",
    "    GOH = round(GOH,3)\n",
    "    #Reduced to MSLP by fixing up pressure now by the gravity constants\n",
    "    Pres_P = Correct_Pres(Pres_T,GOH) \n",
    "    \n",
    "    #Now correct to MSLP\n",
    "    Pres_P['MSLP'] = TO_MSLP(Pres_P, PT_Fixed[1],Elevation)\n",
    "    Pres_P['MSLP'] = pd.to_numeric(Pres_P['MSLP'], errors='coerce')\n",
    "    \n",
    "    #Combine all the data together so you have Date|MSLP|degC|Raw Pressure|Reduced Temp Pressure\n",
    "    Final_Product = pd.concat([Pres_P, PT_Fixed[1]],axis= 1)\n",
    "    Final_Product['degC'] = pd.to_numeric(Final_Product['degC'], errors='coerce')\n",
    "    Final_Product = round(Final_Product,2)\n",
    "    Final_Product = Final_Product[['MSLP','degC',Final_Product.columns[0],Final_Product.columns[1],Final_Product.columns[2]]]\n",
    "    return(Final_Product)\n",
    "\n",
    "\n",
    "## REDUCE TEMPERATURE\n",
    "\n",
    "def Correct_Temp(Pres,Temp,Barometer_Type): #WMO GUIDELINE 1971 3.2.6.3\n",
    "    '''\n",
    "    INPUTS:\n",
    "    -----------------------------\n",
    "    \n",
    "    Pressure_Data : DATAFRAME\n",
    "        Ensure the date is in the index column, pressure it is now in mb and ready to be corrected\n",
    "        \n",
    "    Temperature_Data : DATAFRAME\n",
    "        Ensure the date is in the index column, temperature it is now in degC and ready to be corrected\n",
    "        \n",
    "    Barometer_Type : STRING\n",
    "        What is the type of barometer used for this case. If you do not know use \"Unknown\", but the other barometer \n",
    "        types are \"Fortin\", \"Cistern\", \"Fuess\" and \"Kew\"\n",
    "    \n",
    "    OUTPUTS:\n",
    "    ------------------------------\n",
    "    Pres : DATAFRAME\n",
    "        Dataframe with reduced to temperature pressure in millibars and also the original pressure is still kept as \n",
    "        well\n",
    "    \n",
    "    '''\n",
    "    #If and else statements to ensure we have the barometer of choice, see WMO Guidelines 1971 for \n",
    "    #how these equations are built\n",
    "    \n",
    "    if Barometer_Type == 'Unknown':\n",
    "        #Make it a fortin style barometer\n",
    "        CT = -0.000163*Pres[Pres.columns[0]]*Temp[Temp.columns[0]]\n",
    "        \n",
    "    elif Barometer_Type == 'Fortin':\n",
    "        CT = -0.000163*Pres[Pres.columns[0]]*Temp[Temp.columns[0]]\n",
    "        \n",
    "    elif Barometer_Type == 'Cistern':\n",
    "        #Use the average value between Fuess and Kew Pattern if we do not know which one to take \n",
    "        CT = -0.000163*(Pres[Pres.columns[0]]+39)*Temp[Temp.columns[0]]\n",
    "        \n",
    "    elif Barometer_Type == 'Fuess':\n",
    "        CT = -0.000163*(Pres[Pres.columns[0]]+31)*Temp[Temp.columns[0]]\n",
    "        \n",
    "    elif Barometer_Type == 'Kew':\n",
    "        CT = -0.000163*(Pres[Pres.columns[0]]+47)*Temp[Temp.columns[0]]\n",
    "        \n",
    "    else:\n",
    "        #In case there is an issue\n",
    "        X = 'Error Not a Viable Barometer Type'\n",
    "        return(X)\n",
    "    \n",
    "    #Update the column to include reduced temperature\n",
    "    Pres['Reduced Temp Pressure (mb)'] = Pres[Pres.columns[0]] + CT\n",
    "    \n",
    "    return(Pres)\n",
    "\n",
    "\n",
    "#Gravity calculator\n",
    "\n",
    "def calculate_gravity(latitude, elevation, elevation_type): #WMO GUIDELINE 3.8\n",
    "    \n",
    "    '''\n",
    "    INPUTS:\n",
    "    --------------------\n",
    "    \n",
    "    latitude : RATIONAL NUMBER\n",
    "        Where is the location of the barometer to best possible coordinate value you can find.\n",
    "    \n",
    "    elevation_feet : RATIONAL NUMBER\n",
    "        What is the elevation of the barometer to the best possible knowledge in feet (ft)\n",
    "    \n",
    "    elevation_type : STRING\n",
    "        What is the units of the elevation, is it meters/metres, m, ft or feet\n",
    "        \n",
    "    OUTPUTS:\n",
    "    -------------------------------\n",
    "    \n",
    "    g : RATIONAL NUMBER\n",
    "        Calculates the local gravity from the specific height in metres.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Convert feet to meters\n",
    "    if elevation_type == 'feet':   \n",
    "        elevation_meters = elevation * 0.3048\n",
    "    elif elevation_type == 'metres':   \n",
    "        elevation_meters = elevation\n",
    "    elif elevation_type == 'meters':   \n",
    "        elevation_meters = elevation\n",
    "    elif elevation_type == 'ft':   \n",
    "        elevation_meters = elevation * 0.3048\n",
    "    elif elevation_type == 'm':   \n",
    "        elevation_meters = elevation\n",
    "    else:\n",
    "        X = 'Not a viable unit for elevation'\n",
    "        Y = 0\n",
    "        return(X,Y)\n",
    "    '''\n",
    "    #Equation itself from original, now I have fixed up this gravity equation in the same form\n",
    "    as the WMO guidelines, and assume H = H'\n",
    "    A = 0.0026373#0.0053024\n",
    "    B = 0.0000059#0.0000058\n",
    "\n",
    "    L = math.radians(latitude)  # Convert latitude to radians\n",
    "    #3.086e-6 \n",
    "    g = (9.80616 * (1 + A * math.sin(L)**2 - B * math.sin(2 * L))) - (0.0003086 * elevation_meters)\n",
    "    '''\n",
    "    A = 0.0026373\n",
    "    B = 0.0000059\n",
    "    L = math.radians(latitude)\n",
    "    \n",
    "    g = (980.616 * (1 - (A * math.cos(2 * L)) + (B * math.cos(L)**2))) - (0.0003086 * elevation_meters)   \n",
    "    \n",
    "    return(g,elevation_meters)\n",
    "\n",
    "#Reduce gravity\n",
    "def Correct_Pres(Pres,GOH): #WMO GUIDELINE 1971 3.2.6.2\n",
    "    '''\n",
    "    INPUTS:\n",
    "    --------------------\n",
    "    \n",
    "    Pres : DATAFRAME\n",
    "        Dataframe with reduced to temperature pressure in millibars and also the original pressure is still kept as \n",
    "        well\n",
    "    \n",
    "    GOH : RATIONAL NUMBER\n",
    "        Calculates the local gravity from the specific height in metres.    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    -------------------------------\n",
    "    Pres : DATAFRAME\n",
    "        Dataframe reduced to temperature and gravity in millibars, sicne its 1:1 with hPa, this means this is the \n",
    "        MSLP value to. Reduced to temperature pressure in millibars and also the original pressure is still kept as \n",
    "        well\n",
    "    '''\n",
    "    \n",
    "    GN = 980.665 #cms-2 OUTLINED BY 3.2.6.2\n",
    "    Pres['Correct_Pres_Temp'] = Pres[Pres.columns[1]]*(GOH/GN)#\n",
    "    return(Pres)\n",
    "\n",
    "#Fix up the gravity correction from the WMO guidelines - if only gravity correciton is required\n",
    "def Correct_Pres_1_col_Only(Pres,GOH): #WMO GUIDELINE 1971 3.2.6.2\n",
    "    GN = 980.665 #cms-2 OUTLINED BY 3.2.6.2\n",
    "    Pres['Correct_temp_gravity_mslp'] = Pres[Pres.columns[0]]*(GOH/GN)#\n",
    "    return(Pres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5964c4c-04f8-4c5c-b0a0-65ef5780c80e",
   "metadata": {},
   "source": [
    "# UNIT CONVERTORS FOR TEMPERATURE AND PRESSURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e77e5b60-56e1-4e20-88e3-41cf88abddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################\n",
    "#Convert units to the WMO guidelines\n",
    "\n",
    "def Converters(Pressure_Data,Temperature_Data,Pressure_Unit,Temperature_Unit):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    -------------------------\n",
    "    \n",
    "     Pressure_Data : DATAFRAME\n",
    "        Ensure the date is in the index column, pressure can be any of the typical units used, and make sure it\n",
    "        has a label\n",
    "        \n",
    "    Temperature_Data : DATAFRAME\n",
    "        Ensure the date is in the index column, temperature can be any of the typical units used, and make sure it\n",
    "        has a label\n",
    "        \n",
    "    Pressure_Unit : STRING\n",
    "        What is the pressure units you use. It is \"inHg\", \"mmHg\", \"mb\", \"bar\", \"hPa\", 'Pa', or 'atm'\n",
    "    \n",
    "    Temperature_Unit : STRING\n",
    "        What is the temperature units you use. It is \"degF\", \"degC\", or \"K\"\n",
    "    \n",
    "    OUTPUTS:\n",
    "    ----------------------------\n",
    "    \n",
    "    Pressure_Data : DATAFRAME\n",
    "        This is now an updated version of the pressure unit to WMO Guidelines for the implementation into the\n",
    "        reduction to MSLP. This unit is mb\n",
    "    \n",
    "    Temperature_Data : DATAFRAME\n",
    "        This is now an updated version of the temperature unit to WMO Guidelines for the implementation into the\n",
    "        reduction to MSLP. This unit is degC\n",
    "    '''\n",
    "    #We need a bunch of if else statements\n",
    "    #Pressure Conversion using if and else statement\n",
    "    if Pressure_Unit == 'inHg':\n",
    "        Pressure_Data = Pressure_Data*33.86 #In previous versions, this was 33.38338..., we have not restricted most values to 4-5 singificant figures\n",
    "        \n",
    "    elif Pressure_Unit == 'mmHg':\n",
    "        Pressure_Data = Pressure_Data*1.333\n",
    "        \n",
    "    elif Pressure_Unit == 'bar':\n",
    "        Pressure_Data = Pressure_Data*1000\n",
    "    \n",
    "    elif Pressure_Unit == 'mb':\n",
    "        Pressure_Data = Pressure_Data*1\n",
    "        \n",
    "    elif Pressure_Unit == 'hPa':\n",
    "        Pressure_Data = Pressure_Data*1\n",
    "\n",
    "    elif Pressure_Unit == 'Pa':\n",
    "        Pressure_Data = Pressure_Data*0.01\n",
    "\n",
    "    elif Pressure_Unit == 'atm':\n",
    "        Pressure_Data = Pressure_Data*1013.25 #only expection to the significant features rule\n",
    "        \n",
    "    else:\n",
    "        #This cuts the function from progressing \n",
    "        X ='Error Not a Viable Pressure Unit'\n",
    "        return(X)\n",
    "    \n",
    "    #Temperature Conversion\n",
    "    if Temperature_Unit == 'degC':\n",
    "        Temperature_Data = Temperature_Data*1\n",
    "        \n",
    "    elif Temperature_Unit == 'degF':\n",
    "        Temperature_Data = (Temperature_Data-32)*(5/9)\n",
    "        \n",
    "    elif Temperature_Unit == 'K':\n",
    "        Temperature_Data = Temperature_Data-273.15\n",
    "        \n",
    "    else:\n",
    "        #This cuts the function from progressing \n",
    "        X = 'Error Not a Viable Temperature Unit'\n",
    "        \n",
    "        return(X)\n",
    "   \n",
    "    Pressure_Data.rename(columns = {Pressure_Data.columns[0]:'Raw mb'}, inplace= True)\n",
    "    Temperature_Data.rename(columns = {Temperature_Data.columns[0]:'degC'}, inplace= True)\n",
    "    \n",
    "    #Put together to make it easier when displaying incorrect issues\n",
    "    PT = [Pressure_Data,Temperature_Data]\n",
    "    \n",
    "    return(PT)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c762cd-cad8-4866-860d-fd83d9dd24ff",
   "metadata": {},
   "source": [
    "# LOCAL TO MSLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97733bd-72f3-45b7-915e-6b1acb89a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TO_MSLP(Pres, Temp, elevation):\n",
    "    '''\n",
    "    Calculate Mean Sea Level Pressure (MSLP) using the Hypsometric equation.\n",
    "    \n",
    "    Args:\n",
    "    - Pres: Pressure data (already corrected for temperature).\n",
    "    - Temp: Temperature data in degrees Celsius.\n",
    "    - elevation: Elevation in meters.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated pressure data with MSLP.\n",
    "    '''\n",
    "    \n",
    "    A = elevation * 9.81 \n",
    "    B = 287 * (Temp['degC'] + 273.15)\n",
    "    C = A/B\n",
    "    C = pd.to_numeric(C, errors='coerce')\n",
    "    D = np.exp(C)\n",
    "    MSLP = Pres['Correct_Pres_Temp'] * D\n",
    "    return MSLP\n",
    "\n",
    "\n",
    "def TO_SP(MSLP, Temp, elevation):\n",
    "    '''\n",
    "    Calculate Mean Sea Level Pressure (MSLP) using the Hypsometric equation.\n",
    "    \n",
    "    Args:\n",
    "    - Pres: Pressure data (already corrected for temperature).\n",
    "    - Temp: Temperature data in degrees Celsius.\n",
    "    - elevation: Elevation in meters.\n",
    "    \n",
    "    Returns:\n",
    "    - Updated pressure data with MSLP.\n",
    "    '''\n",
    "    \n",
    "    A = elevation * 9.81 \n",
    "    B = 287 * (Temp['degC'] + 273.15)\n",
    "    C = A/B\n",
    "    C = pd.to_numeric(C, errors='coerce')\n",
    "    D = np.exp(C)\n",
    "\n",
    "    SP = MSLP/D\n",
    "    return SP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415d3aa-6be3-42f4-8fff-7d97e2cb1118",
   "metadata": {},
   "source": [
    "# RHTEST CONVERT DATA CONVERTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8a6d67-fbce-4c27-bb67-bf749b468396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RHTest adjustments WE\n",
    "def rh_fmt_row_month_mean_via_WMO_Guide(df_old,Raindays = False):\n",
    "    \"\"\" This will transform which has a row per observing time into a format readable by the RH test software\n",
    "    \n",
    "    Args:\n",
    "    df: dataframe with a row per observing time\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = df_old.reset_index().columns #'Date','MSLP'\n",
    "\n",
    "    #extract the monthlies under WMO guidelines \n",
    "    df = WMO_Guide_Months(df_old,columns[0],columns[1],Raindays)\n",
    "\n",
    "    #fill any nans as -999.99\n",
    "    df.fillna(-999.99,inplace=True)   \n",
    "\n",
    "    #Extract year month and set day to 0\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = 0\n",
    "\n",
    "    #Reset the index to remmove\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-3:] + cols[:-3] #reorder column order so year month day appear first\n",
    "    df = df[cols]\n",
    "    \n",
    "    newdf = df[['year','month','day','MSLP']]\n",
    "    \n",
    "    return newdf\n",
    "\n",
    "'''\n",
    "The second RHTest format ensures any periods where only monthlies are avalaible can be used\n",
    "'''\n",
    "\n",
    "def rh_fmt_row_month_mean_all(df_old):\n",
    "    \"\"\" This will transform which has a row per observing time into a format readable by the RH test software\n",
    "    \n",
    "    Args:\n",
    "    df: dataframe with a row per observing time\n",
    "    \"\"\"\n",
    "    df = df_old\n",
    "    columns = df_old.columns #'MSLP'\n",
    "    \n",
    "    df = df.resample('MS').mean()\n",
    "\n",
    "    #We create the full index for the daily data with the monthly data first full month and last month\n",
    "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='MS')\n",
    "    # Reindex the original DataFrame to this complete index\n",
    "    df = df.reindex(full_index).round(1)\n",
    "    df.fillna(-999.99,inplace=True)   \n",
    "\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = 0\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-3:] + cols[:-3] #reorder column order so year month day appear first\n",
    "    df = df[cols]\n",
    "    \n",
    "    newdf = df[['year','month','day','MSLP']]\n",
    "    \n",
    "    return newdf\n",
    "\n",
    "'''\n",
    "The third rhtest, is just the daily variant\n",
    "'''\n",
    "def rh_fmt_row_day_mean(df_old):\n",
    "    \"\"\" This will transform which has a row per observing time into a format readable by the RH test software\n",
    "    \n",
    "    Args:\n",
    "    df: dataframe with a row per observing time\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_old\n",
    "    #We create the full index for the daily data with the monthly data first full month and last month\n",
    "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    # Reindex the original DataFrame to this complete index\n",
    "    df = df.reindex(full_index).round(1)\n",
    "    df.fillna(-999.99,inplace=True)   \n",
    "\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df['day'] = df.index.day\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-3:] + cols[:-3] #reorder column order so year month day appear first\n",
    "    df = df[cols]\n",
    "    \n",
    "    newdf = df[['year','month','day','MSLP']]\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd5017-5721-44cf-a310-97df231c02b3",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION EQUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4d2c0ab-f647-4fb6-9604-4fb63a6ac0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Regression_Equations(Trials, hours, Data):\n",
    "    '''\n",
    "    Parameters\n",
    "    --------------\n",
    "    Trials : Integer\n",
    "        The number of trails you want to run the estimation training over.\n",
    "        \n",
    "    hours : array\n",
    "    \n",
    "    Data : DataFrame/Dictionary\n",
    "        Using the observations and the training data we can have created a dictionary of DataFrames\n",
    "        that have trialed that have been sampled by the lenght of the data avalaible for that month.\n",
    "    \n",
    "    Returns\n",
    "    --------------\n",
    "    Regressed_Trial : Dictionary/DataFrames\n",
    "        For each trial, the set of equations for each sub-daily time, Trained DEs and Month will be generated for application \n",
    "        to estimate the DEs of the inputted data for estimation.\n",
    "    '''\n",
    "    \n",
    "    #Create dictionaries\n",
    "    Regressed_Trial = {}\n",
    "    \n",
    "    #Define the month names\n",
    "    Month_Name = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "    #Being the for loop by extracting the month name\n",
    "    for month_num in range(0,12):\n",
    "        #Extract the month name_\n",
    "        Month_Str =  Month_Name[month_num]\n",
    "        #This is useful in the key as it is Aug_9_Mn_Samp where month_hour_mx/mn_samp\n",
    "        \n",
    "        #Now using the trials lets extract the trials within the particular dictionary\n",
    "        for trial_number in range(1,Trials+1):\n",
    "            #Now this is all the arrays that will be appended to at the end that include the linear\n",
    "            #regression line components, A and B and the Correlation by the spearman r\n",
    "            AMx_Total = []\n",
    "            BMx_Total = []\n",
    "            CORRMx_Total = []\n",
    "            Time = []\n",
    "            AMn_Total = []\n",
    "            BMn_Total = []\n",
    "            CORRMn_Total = []\n",
    "            \n",
    "            #Now for loop to extract the data and get the regression\n",
    "            for i in hours:\n",
    "                #---MAX---#\n",
    "                #Extract the maximum data\n",
    "                Mxt = Data.get('{}_{}_Mx_Samp'.format(Month_Str,i))\n",
    "                #Get the linear formula and the correlation of the data\n",
    "                AMx, BMx, corrMx = linear_regression_polyfit(Mxt['temp Run {}'.format(trial_number)],Mxt['Max Run {}'.format(trial_number)])\n",
    "                #Append it all\n",
    "                AMx_Total.append(AMx)\n",
    "                BMx_Total.append(BMx)\n",
    "                CORRMx_Total.append(corrMx)\n",
    "                #Repeat for min\n",
    "                #---MIN---#\n",
    "                Mnt = Data.get('{}_{}_Mn_Samp'.format(Month_Str,i))\n",
    "                AMn, BMn, corrMn = linear_regression_polyfit(Mnt['temp Run {}'.format(trial_number)],Mnt['Min Run {}'.format(trial_number)])\n",
    "                Time.append(int(i)) \n",
    "                AMn_Total.append(AMn)\n",
    "                BMn_Total.append(BMn)\n",
    "                CORRMn_Total.append(corrMn)\n",
    "\n",
    "            #Add it all into a dataframe\n",
    "            Time = pd.Series(Time,name = 'Hours')\n",
    "            \n",
    "            AMX = pd.Series(AMx_Total,name = 'A')\n",
    "            BMX = pd.Series(BMx_Total,name = 'B')\n",
    "            corrMX = pd.Series(CORRMx_Total,name = 'Correlation')\n",
    "            ItemsMX = pd.concat([Time,AMX,BMX,corrMX],axis = 1)\n",
    "            \n",
    "            AMN = pd.Series(AMn_Total,name = 'A')\n",
    "            BMN = pd.Series(BMn_Total,name = 'B')\n",
    "            corrMN = pd.Series(CORRMn_Total,name = 'Correlation')\n",
    "            ItemsMN = pd.concat([Time,AMN,BMN,corrMN],axis = 1)\n",
    "            \n",
    "            Regressed_Trial[\"{}\".format(Month_Str) + \"_\" + 'Trial'+ \"_\" + str(trial_number) + \"_\" + \"Mx\"] = ItemsMX\n",
    "            Regressed_Trial[\"{}\".format(Month_Str) + \"_\" + 'Trial'+ \"_\" + str(trial_number) + \"_\" + \"Mn\"] = ItemsMN\n",
    "    return(Regressed_Trial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now develop the linear regression equation\n",
    "def linear_regression_polyfit(x,y):\n",
    "    #Find the linear Relationship\n",
    "    A, B = np.polyfit(x, y, 1)\n",
    "    #Find the correlation                  \n",
    "    corr, _ = spearmanr(x, y)\n",
    "    return(A,B,corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66314f5-c697-4c45-8210-d063dc8011e1",
   "metadata": {},
   "source": [
    "# ZAK QM/PPM FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3577f317-5a23-43d3-9250-31920968c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get Zaks QM adjustment code from the data to esnure we can do this and correct the Perth Metro to Perth Airport\n",
    "def qm_transfer(reference,uncorrected,value):\n",
    "    #reference = vector of values representing the percentiles of the reference data\n",
    "    #uncorrected = vector of values representing the percentiles of the data to be corrected\n",
    "    #value = Value to be adjusted\n",
    "    \n",
    "    #  \"\"\"Use source and destination percentiles to get the homogenised value.\n",
    "    ## From Linden Ashcroft's R script (adapted from BoM Python script)\n",
    "    #Given two lists of values representing, for example,\n",
    "    #temperatures at the 5th to 95th percentiles, and a value\n",
    "    #representing an amount to be homogenised, return the\n",
    "    #homogenised value.\n",
    "    #\"\"\"\n",
    "    \n",
    "    if pd.isna(value):\n",
    "        return(np.nan)\n",
    "    \n",
    "    if value < round(uncorrected.iloc[0],3):\n",
    "        return value + (reference.iloc[0] - uncorrected.iloc[0])\n",
    "    \n",
    "        #if the value is less than the lowest percentile, \n",
    "        #adjustment is difference between\n",
    "        #the lowest percentile of the two\n",
    "    \n",
    "    if value > round(uncorrected.iloc[-1],3):\n",
    "        return value + (reference.iloc[-1] - uncorrected.iloc[-1])\n",
    "    \n",
    "        #if the value is greater than the highest percentile, \n",
    "        #adjustment is difference between\n",
    "        #the highest percentile of the two\n",
    "    \n",
    "    ndx = min(bisect_left(np.array(uncorrected), value), len(uncorrected) - 1)\n",
    "    \n",
    "    num_equal = np.count_nonzero(np.array(round(uncorrected,3) == value))\n",
    "    \n",
    "    if num_equal == 1:\n",
    "        return value + (reference.iloc[ndx] - uncorrected.iloc[ndx])\n",
    "        #If the value is an exact percentile value, \n",
    "        #then the adjustment is the difference between that percentile\n",
    "    elif num_equal > 1:\n",
    "        offset = random.randint(1, num_equal) - 1\n",
    "        return value + (reference.iloc[ndx + offset] - uncorrected.iloc[ndx + offset])\n",
    "    \n",
    "        #If there are two percentiles with the value, \n",
    "        #pick one at random and use that to find the adjustment\n",
    "        \n",
    "    else:\n",
    "        return (((reference.iloc[ndx] - reference.iloc[ndx-1])/\n",
    "                   (uncorrected.iloc[ndx] - uncorrected.iloc[ndx-1]))*\n",
    "                  (value - uncorrected.iloc[ndx-1]) + reference.iloc[ndx-1])\n",
    "    \n",
    "        #Otherwise, find the percentiles 'bin' that the value fits in, \n",
    "        #subtract the Glaisher percentile value from the value of interest\n",
    "        #multiply that by a ratio of the two percentiles \n",
    "        #(if they are changing at the same rate this term will be 1) \n",
    "        #and add the Stevenson screen value for that percentile value\n",
    "        \n",
    "        #JR input, dont worry about the glaisher stuff, our focus is\n",
    "        #on the pressure and this is something that can be similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d7078-6fa3-42e8-aa2d-03f8821b3f26",
   "metadata": {},
   "source": [
    "# COMBINING PERTHS DATASETS TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66d09ed0-aaf6-48cf-9151-f251431bfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perth_Combination(PM_MSLP,PA_MSLP,PRO_MSLP_B,PRO_MSLP,PO_MSLP,PG_MONTHLY,PG_MSLP,SWR_MSLP):\n",
    "    ######################### PA TO PM\n",
    "    \n",
    "    #using Zaks code step by step Testing how 2 and 5 years impact the final product\n",
    "    historical = PA_MSLP[['MSLP']].loc[:'1996-01-11']\n",
    "    modern = PM_MSLP[['MSLP']].loc['1994-01-12':'1996-01-11']\n",
    "    idx = historical.index.intersection(modern.index)\n",
    "    modern_overlap = modern.loc[idx]\n",
    "    historical_overlap = historical.loc[idx]\n",
    "    \n",
    "    timestamp = historical.columns\n",
    "    correctedDf = historical.copy()\n",
    "    for col in historical:\n",
    "        historicalPct = historical_overlap[col].quantile(np.arange(0.05,1,0.05)) #calculate the percentiles for both datasets \n",
    "        modernPct = modern_overlap[col].quantile(np.arange(0.05,1,0.05))\n",
    "        correctedList = []\n",
    "        for j in range(len(historical[col])):\n",
    "            correctedList.append(qm_transfer(modernPct,historicalPct,historical[col][j])) #loop through each value and correct\n",
    "        correctedDf[col + '_corrected'] = correctedList\n",
    "        del historicalPct, modernPct, correctedList\n",
    "    corrected = correctedDf.drop(correctedDf.columns[0:len(historical.columns)], axis=1)\n",
    "    corrected.columns = corrected.columns.str.replace('_corrected','')\n",
    "    \n",
    "    #Perth Metro reference\n",
    "    PERTH_Pres = PM_MSLP[['MSLP']]\n",
    "    \n",
    "    #Add Perth Airport Corrected to PER_011_P_MSLP\n",
    "    \n",
    "    PERTH_Pres =  pd.concat([corrected[:'1994-01-11'],PERTH_Pres], axis =0)\n",
    "    \n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    ######################### PRO B TO PM\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "\n",
    "    #using Zaks code step by step Testing how 2 and 5 years impact the final product\n",
    "    historical = PRO_MSLP_B[['MSLP']].loc[:'1992-04-10']\n",
    "    modern = PERTH_Pres.loc['1990-04-11':'1992-04-10']\n",
    "    idx = historical.index.intersection(modern.index)\n",
    "    modern_overlap = modern.loc[idx]\n",
    "    historical_overlap = historical.loc[idx]\n",
    "    \n",
    "    timestamp = historical.columns\n",
    "    correctedDf = historical.copy()\n",
    "    for col in historical:\n",
    "        historicalPct = historical_overlap[col].quantile(np.arange(0.05,1,0.05)) #calculate the percentiles for both datasets \n",
    "        modernPct = modern_overlap[col].quantile(np.arange(0.05,1,0.05))\n",
    "        correctedList = []\n",
    "        for j in range(len(historical[col])):\n",
    "            correctedList.append(qm_transfer(modernPct,historicalPct,historical[col][j])) #loop through each value and correct\n",
    "        correctedDf[col + '_corrected'] = correctedList\n",
    "        del historicalPct, modernPct, correctedList\n",
    "    corrected = correctedDf.drop(correctedDf.columns[0:len(historical.columns)], axis=1)\n",
    "    corrected.columns = corrected.columns.str.replace('_corrected','')\n",
    "    \n",
    "    #Add Perth Regional Office Corrected to PER_011_P_MSLP\n",
    "    \n",
    "    PERTH_Pres =  pd.concat([corrected,PERTH_Pres['1992-04-11':]], axis =0)\n",
    "    \n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    ######################### PRO U TO PM\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    \n",
    "    historical = PRO_MSLP.loc[:'1944-03-31']\n",
    "    modern = PERTH_Pres.loc['1942-04-01':'1944-03-31']\n",
    "    idx = historical.index.intersection(modern.index)\n",
    "    modern_overlap = modern.loc[idx]\n",
    "    historical_overlap = historical.loc[idx]\n",
    "    \n",
    "    timestamp = historical.columns\n",
    "    correctedDf = historical.copy()\n",
    "    for col in historical:\n",
    "        historicalPct = historical_overlap[col].quantile(np.arange(0.05,1,0.05)) #calculate the percentiles for both datasets \n",
    "        modernPct = modern_overlap[col].quantile(np.arange(0.05,1,0.05))\n",
    "        correctedList = []\n",
    "        for j in range(len(historical[col])):\n",
    "            correctedList.append(qm_transfer(modernPct,historicalPct,historical[col][j])) #loop through each value and correct\n",
    "        correctedDf[col + '_corrected'] = correctedList\n",
    "        del historicalPct, modernPct, correctedList\n",
    "    corrected = correctedDf.drop(correctedDf.columns[0:len(historical.columns)], axis=1)\n",
    "    corrected.columns = corrected.columns.str.replace('_corrected','')\n",
    "    \n",
    "    #Add Perth Regional Office Corrected to PERTH_Pres\n",
    "    \n",
    "    PERTH_Pres =  pd.concat([corrected[:'1942-03-31'],PERTH_Pres.loc['1942-04-01':]], axis =0)\n",
    "    \n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    ######################### PO TO PM\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    \n",
    "    #using Zaks code step by step Testing how 2 and 5 years impact the final product\n",
    "    historical = PO_MSLP.sort_index().loc[:'1908-12-31']\n",
    "    modern = PERTH_Pres.sort_index().loc['1907-01-01':'1908-12-31']\n",
    "    idx = historical.index.intersection(modern.index)\n",
    "    modern_overlap = modern.loc[idx]\n",
    "    historical_overlap = historical.loc[idx]\n",
    "    \n",
    "    timestamp = historical.columns\n",
    "    correctedDf = historical.copy()\n",
    "    for col in historical:\n",
    "        historicalPct = historical_overlap[col].quantile(np.arange(0.05,1,0.05)) #calculate the percentiles for both datasets \n",
    "        modernPct = modern_overlap[col].quantile(np.arange(0.05,1,0.05))\n",
    "        correctedList = []\n",
    "        for j in range(len(historical[col])):\n",
    "            correctedList.append(qm_transfer(modernPct,historicalPct,historical[col][j])) #loop through each value and correct\n",
    "        correctedDf[col + '_corrected'] = correctedList\n",
    "        del historicalPct, modernPct, correctedList\n",
    "    corrected = correctedDf.drop(correctedDf.columns[0:len(historical.columns)], axis=1)\n",
    "    corrected.columns = corrected.columns.str.replace('_corrected','')\n",
    "    \n",
    "    #Add Perth Regional Office Corrected to PER_011_P_MSLP\n",
    "    \n",
    "    PERTH_Pres =  pd.concat([corrected[:'1906-12-31'],PERTH_Pres], axis =0)\n",
    "    \n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    ######################### PG TO PM\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    \n",
    "    #using Zaks code step by step Testing how 2 and 5 years impact the final product\n",
    "    historical = PG_MSLP.sort_index().loc['1880':'1898-12-31']\n",
    "    modern = PERTH_Pres.sort_index().loc['1897-01-01':'1898-12-31']\n",
    "    idx = historical.index.intersection(modern.index)\n",
    "    modern_overlap = modern.loc[idx]\n",
    "    historical_overlap = historical.loc[idx]\n",
    "    \n",
    "    timestamp = historical.columns\n",
    "    correctedDf = historical.copy()\n",
    "    for col in historical:\n",
    "        historicalPct = historical_overlap[col].quantile(np.arange(0.05,1,0.05)) #calculate the percentiles for both datasets \n",
    "        modernPct = modern_overlap[col].quantile(np.arange(0.05,1,0.05))\n",
    "        correctedList = []\n",
    "        for j in range(len(historical[col])):\n",
    "            correctedList.append(qm_transfer(modernPct,historicalPct,historical[col][j])) #loop through each value and correct\n",
    "        correctedDf[col + '_corrected'] = correctedList\n",
    "        del historicalPct, modernPct, correctedList\n",
    "    corrected = correctedDf.drop(correctedDf.columns[0:len(historical.columns)], axis=1)\n",
    "    corrected.columns = corrected.columns.str.replace('_corrected','')\n",
    "    \n",
    "    #Add Perth Regional Office Corrected to PER_011_P_MSLP\n",
    "    \n",
    "    PERTH_Pres =  pd.concat([corrected[:'1896-12-31'],PERTH_Pres], axis =0)\n",
    "\n",
    "    \n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    ######################### PG Monthly TO PM\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    \n",
    "\n",
    "    #using Zaks code step by step Testing how 2 and 5 years impact the final product, this is using a monthly adjustment\n",
    "    historical = PG_MONTHLY.sort_index().loc[:'1881-12']\n",
    "    modern = PERTH_Pres.sort_index().loc['1880-01-01':'1881-12-31'].resample('D').mean().resample('MS').mean()\n",
    "    idx = historical.index.intersection(modern.index)\n",
    "    modern_overlap = modern.loc[idx]\n",
    "    historical_overlap = historical.loc[idx]\n",
    "    \n",
    "    timestamp = historical.columns\n",
    "    correctedDf = historical.copy()\n",
    "    for col in historical:\n",
    "        historicalPct = historical_overlap[col].quantile(np.arange(0.05,1,0.05)) #calculate the percentiles for both datasets \n",
    "        modernPct = modern_overlap[col].quantile(np.arange(0.05,1,0.05))\n",
    "        correctedList = []\n",
    "        for j in range(len(historical[col])):\n",
    "            correctedList.append(qm_transfer(modernPct,historicalPct,historical[col][j])) #loop through each value and correct\n",
    "        correctedDf[col + '_corrected'] = correctedList\n",
    "        del historicalPct, modernPct, correctedList\n",
    "    corrected = correctedDf.drop(correctedDf.columns[0:len(historical.columns)], axis=1)\n",
    "    corrected.columns = corrected.columns.str.replace('_corrected','')\n",
    "    \n",
    "    #Add Perth Regional Office Corrected to PER_011_P_MSLP\n",
    "    \n",
    "    PERTH_Pres =  pd.concat([corrected[:'1879-12-31'],PERTH_Pres], axis =0)\n",
    "    \n",
    "    \n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    ######################### SWR TO PM\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    #########################\n",
    "    \n",
    "    #Start by extracting data from that month only with the new monthly dataset, therefore the mean adjustment is from \n",
    "    PER_1880_1909 = PERTH_Pres.resample('D').mean().resample('MS').mean().loc['1876':'1905'].groupby([PERTH_Pres.resample('D').mean().resample('MS').mean().loc['1876':'1905'].index.month]).mean().reset_index().rename(columns = {'Date':'Month'}).set_index('Month')\n",
    "    \n",
    "    \n",
    "    PER_1852_1875 = SWR_MSLP.loc['1852':'1875'].groupby([SWR_MSLP.loc['1852':'1875'].index.month]).mean().reset_index().rename(columns = {'Date':'Month'}).set_index('Month')\n",
    "    \n",
    "    \n",
    "    PER_1830_1851 = SWR_MSLP.loc['1830':'1851'].groupby([SWR_MSLP.loc['1830':'1851'].index.month]).mean().reset_index().rename(columns = {'Date':'Month'}).set_index('Month')\n",
    "    \n",
    "    PER_1830_1875 = SWR_MSLP.loc['1830':'1875'].groupby([SWR_MSLP.loc['1830':'1875'].index.month]).mean().reset_index().rename(columns = {'Date':'Month'}).set_index('Month')\n",
    "    \n",
    "    #Get the difference\n",
    "    PER_30_51_Dif = SWR_MSLP.loc['1830':'1851'].copy().reset_index()\n",
    "    PER_30_51_Dif = PER_30_51_Dif.drop_duplicates(subset='Date', keep='last').set_index('Date')\n",
    "    \n",
    "    \n",
    "    date = []\n",
    "    pressure_dif = []\n",
    "    for i in PER_30_51_Dif.index:\n",
    "        #Extract month\n",
    "        month = i.month\n",
    "        \n",
    "        if not np.isnan(PER_30_51_Dif.loc[\"{}\".format(i)].values):\n",
    "            date.append(i)\n",
    "            pressure_dif.append(PER_30_51_Dif.loc[\"{}\".format(i)].values[0]  - PER_1830_1851.loc[month].values[0])\n",
    "        else:\n",
    "            date.append(i)\n",
    "            pressure_dif.append(np.nan)\n",
    "            \n",
    "    # Convert the list of timestamps to a pandas DataFrame\n",
    "    date = pd.DataFrame({'Date': date})\n",
    "    pressure_dif = pd.DataFrame({'MSLP_DIF': pressure_dif})\n",
    "    \n",
    "        \n",
    "    PER_30_51_Dif = pd.concat([date,pressure_dif],axis = 1).set_index('Date')\n",
    "    \n",
    "    \n",
    "    #########################################################\n",
    "    #Now add that onto the average of the 1880-1909 plot\n",
    "    date = []\n",
    "    pressure_mean_adj = []\n",
    "    for i in PER_30_51_Dif.index:\n",
    "        #Extract month\n",
    "        month = i.month\n",
    "        \n",
    "        if not np.isnan(PER_30_51_Dif.loc[\"{}\".format(i)].values):\n",
    "            date.append(i)\n",
    "            pressure_mean_adj.append(PER_30_51_Dif.loc[\"{}\".format(i)].values[0]  + PER_1880_1909.loc[month].values[0])\n",
    "        else:\n",
    "            date.append(i)\n",
    "            pressure_mean_adj.append(np.nan)\n",
    "            \n",
    "    # Convert the list of timestamps to a pandas DataFrame\n",
    "    date = pd.DataFrame({'Date': date})\n",
    "    pressure_mean_adj = pd.DataFrame({'MSLP': pressure_mean_adj})\n",
    "    \n",
    "        \n",
    "    PER_30_51_AJD = pd.concat([date,pressure_mean_adj],axis = 1).set_index('Date')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #####################################################################################3333\n",
    "    \n",
    "    #Get the difference\n",
    "    PER_52_75_Dif = SWR_MSLP.loc['1852':'1875'].copy().reset_index()\n",
    "    PER_52_75_Dif = PER_52_75_Dif.drop_duplicates(subset='Date', keep='last').set_index('Date')\n",
    "    date = []\n",
    "    pressure_dif = []\n",
    "    for i in PER_52_75_Dif.index:\n",
    "        #Extract month\n",
    "        month = i.month\n",
    "        \n",
    "        if not np.isnan(PER_52_75_Dif.loc[\"{}\".format(i)].values):\n",
    "            date.append(i)\n",
    "            pressure_dif.append(PER_52_75_Dif.loc[\"{}\".format(i)].values[0]  - PER_1852_1875.loc[month].values[0])\n",
    "        else:\n",
    "            date.append(i)\n",
    "            pressure_dif.append(np.nan)\n",
    "            \n",
    "    # Convert the list of timestamps to a pandas DataFrame\n",
    "    date = pd.DataFrame({'Date': date})\n",
    "    pressure_dif = pd.DataFrame({'MSLP_DIF': pressure_dif})\n",
    "    \n",
    "        \n",
    "    PER_52_75_Dif = pd.concat([date,pressure_dif],axis = 1).set_index('Date')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #########################################################\n",
    "    #Now add that onto the average of the 1880-1909 plot\n",
    "    date = []\n",
    "    pressure_mean_adj = []\n",
    "    for i in PER_52_75_Dif.index:\n",
    "        #Extract month\n",
    "        month = i.month\n",
    "        \n",
    "        if not np.isnan(PER_52_75_Dif.loc[\"{}\".format(i)].values):\n",
    "            date.append(i)\n",
    "            pressure_mean_adj.append(PER_52_75_Dif.loc[\"{}\".format(i)].values[0]  + PER_1880_1909.loc[month].values[0])\n",
    "        else:\n",
    "            date.append(i)\n",
    "            pressure_mean_adj.append(np.nan)\n",
    "            \n",
    "    # Convert the list of timestamps to a pandas DataFrame\n",
    "    date = pd.DataFrame({'Date': date})\n",
    "    pressure_mean_adj = pd.DataFrame({'MSLP': pressure_mean_adj})\n",
    "    \n",
    "        \n",
    "    PER_52_75_AJD = pd.concat([date,pressure_mean_adj],axis = 1).set_index('Date')\n",
    "    \n",
    "\n",
    "    #####################################################################################3333\n",
    "    \n",
    "    #Get the difference\n",
    "    PER_30_75_Dif = SWR_MSLP.loc['1830':'1875'].copy().reset_index()\n",
    "    PER_30_75_Dif = PER_30_75_Dif.drop_duplicates(subset='Date', keep='last').set_index('Date')\n",
    "    date = []\n",
    "    pressure_dif = []\n",
    "    for i in PER_30_75_Dif.index:\n",
    "        #Extract month\n",
    "        month = i.month\n",
    "        \n",
    "        if not np.isnan(PER_30_75_Dif.loc[\"{}\".format(i)].values):\n",
    "            date.append(i)\n",
    "            pressure_dif.append(PER_30_75_Dif.loc[\"{}\".format(i)].values[0]  - PER_1830_1875.loc[month].values[0])\n",
    "        else:\n",
    "            date.append(i)\n",
    "            pressure_dif.append(np.nan)\n",
    "            \n",
    "    # Convert the list of timestamps to a pandas DataFrame\n",
    "    date = pd.DataFrame({'Date': date})\n",
    "    pressure_dif = pd.DataFrame({'MSLP_DIF': pressure_dif})\n",
    "    \n",
    "        \n",
    "    PER_30_75_Dif = pd.concat([date,pressure_dif],axis = 1).set_index('Date')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #########################################################\n",
    "    #Now add that onto the average of the 1880-1909 plot\n",
    "    date = []\n",
    "    pressure_mean_adj = []\n",
    "    for i in PER_30_75_Dif.index:\n",
    "        #Extract month\n",
    "        month = i.month\n",
    "        \n",
    "        if not np.isnan(PER_30_75_Dif.loc[\"{}\".format(i)].values):\n",
    "            date.append(i)\n",
    "            pressure_mean_adj.append(PER_30_75_Dif.loc[\"{}\".format(i)].values[0]  + PER_1880_1909.loc[month].values[0])\n",
    "        else:\n",
    "            date.append(i)\n",
    "            pressure_mean_adj.append(np.nan)\n",
    "            \n",
    "    # Convert the list of timestamps to a pandas DataFrame\n",
    "    date = pd.DataFrame({'Date': date})\n",
    "    pressure_mean_adj = pd.DataFrame({'MSLP': pressure_mean_adj})\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    PER_30_75_AJD = pd.concat([date,pressure_mean_adj],axis = 1).set_index('Date')\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ####################################################################\n",
    "    PER_30_75_AJD= pd.concat([PER_30_51_AJD,PER_52_75_AJD],axis = 0)\n",
    "    '''\n",
    "\n",
    "    ##############################################################\n",
    "    \n",
    "    PER_30_24= pd.concat([PER_30_75_AJD,PERTH_Pres],axis = 0)\n",
    "    return(PER_30_24)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd64a42-b834-4515-8e73-67c8bed6e788",
   "metadata": {},
   "source": [
    "# STORM FINDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cfce2d6-12d2-41e2-b68e-14efff7a353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Storm_Finder_AP_2009(Pressure,Raindays_Av,Raindays,Rainfall,Extreme,RD):\n",
    "    \n",
    "    #Fix data so its all filled up with 24hours\n",
    "    All_Dates = pd.date_range(start=Pressure.index.min(), end=Pressure.index.max(), freq='h')\n",
    "    All_Dates = pd.DataFrame(index=All_Dates, columns=[Pressure.columns[0]])\n",
    "    \n",
    "    #Insert the missing hours into the Pressure dataset\n",
    "    Pressure = All_Dates.combine_first(Pressure)\n",
    "    \n",
    "    #Extract 9am and 3pm only\n",
    "    Pressure = Pressure[Pressure.index.hour.isin([9, 15])]\n",
    "    \n",
    "    # Make sure Pressure is sorted by datetime index\n",
    "    Pressure = Pressure.sort_index()\n",
    "    \n",
    "    # Calculate time difference to the next time (in hours)\n",
    "    Pressure[\"Hours_to_next\"] = (Pressure.index.to_series().shift(-1) - Pressure.index.to_series()).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate tendency (difference between current and next MSLP)\n",
    "    Pressure[\"Tendency\"] = Pressure[P_MSLP_SD.columns[0]].shift(-1) - Pressure[Pressure.columns[0]]\n",
    "    #The tendency is calculated on the day of focos so row 1003.0 3.8, means in 6hr its 1006.8\n",
    "    #This is to indentify the change on that particular day\n",
    "    \n",
    "    #Now we need to work out the 1st and 99th percentile tendency values of both 6hr and 18hr\n",
    "    T_6hr = Pressure[Pressure['Hours_to_next'] == 6][['Tendency']]\n",
    "    T_6hr[\"Tendency\"] = pd.to_numeric(T_6hr[\"Tendency\"], errors='coerce')\n",
    "    \n",
    "    T_18hr = Pressure[Pressure['Hours_to_next'] == 18][['Tendency']]\n",
    "    T_18hr[\"Tendency\"] = pd.to_numeric(T_18hr[\"Tendency\"], errors='coerce')\n",
    "    \n",
    "    \n",
    "    #Then work out 1st and 99th percentile for each month\n",
    "    T_6hr['month'] = T_6hr.index.month\n",
    "    T_18hr['month'] = T_18hr.index.month\n",
    "    \n",
    "    #For each month, now we have the range where the most extreme cases occur\n",
    "    percentiles_ind_6hr = T_6hr.dropna().groupby('month')['Tendency'].quantile([0.01, 0.99])\n",
    "    percentiles_ind_18hr = T_18hr.dropna().groupby('month')['Tendency'].quantile([0.01, 0.99])\n",
    "    \n",
    "    # Convert to DataFrame and unstack the quantile level\n",
    "    percentiles_ind_6hr = percentiles_ind_6hr.unstack(level=1)\n",
    "    percentiles_ind_18hr = percentiles_ind_18hr.unstack(level=1)\n",
    "    \n",
    "    # rename columns for clarity\n",
    "    percentiles_ind_6hr.columns = ['1st', '99th']\n",
    "    percentiles_ind_18hr.columns = ['1st', '99th']\n",
    "\n",
    "    print(percentiles_ind_6hr)\n",
    "    print(percentiles_ind_18hr)\n",
    "    \n",
    "    #6hr\n",
    "    date_kept_6 = []\n",
    "    \n",
    "    #Iterate through the entire timeseries\n",
    "    for row in T_6hr.itertuples():\n",
    "        # Extract the month and tendency from the row\n",
    "        mth = row.month\n",
    "        tend = row.Tendency\n",
    "    \n",
    "        #Extract the 99th and 1st percentile\n",
    "        if (tend <= percentiles_ind_6hr['1st'][mth]) | (tend >= percentiles_ind_6hr['99th'][mth]):\n",
    "            date_kept_6.append(row.Index)\n",
    "        else:\n",
    "            x = 0\n",
    "    \n",
    "    #6hr\n",
    "    date_kept_18 = []\n",
    "    \n",
    "    #Iterate through the entire timeseries\n",
    "    for row in T_18hr.itertuples():\n",
    "        # Extract the month and tendency from the row\n",
    "        mth = row.month\n",
    "        tend = row.Tendency\n",
    "    \n",
    "        #Extract the 99th and 1st percentile\n",
    "        if (tend <= percentiles_ind_18hr['1st'][mth]) | (tend >= percentiles_ind_18hr['99th'][mth]):\n",
    "            date_kept_18.append(row.Index)\n",
    "        else:\n",
    "            x = 0\n",
    "    date_kept_6 = pd.DataFrame(date_kept_6, columns=['Date']).set_index('Date')\n",
    "    date_kept_18 = pd.DataFrame(date_kept_18, columns=['Date']).set_index('Date')\n",
    "    \n",
    "    #Combine the potential storms together\n",
    "    Pot_Storms = pd.concat([date_kept_6,date_kept_18],axis = 1).sort_index()\n",
    "    #Merge with the actual tendency and MSLP data\n",
    "    Pot_Storms = pd.merge(Pot_Storms,Pressure,how = 'inner',left_index = True, right_index= True)\n",
    "    \n",
    "    #Remove duplicated dates\n",
    "    Pot_Storms['Date'] = Pot_Storms.index.strftime('%Y-%m-%d')\n",
    "    Pot_Storms = Pot_Storms.drop_duplicates(subset='Date', keep='first')\n",
    "\n",
    "    #ID the storms\n",
    "    # Calculate the difference between consecutive dates\n",
    "    Pot_Storms['Date'] = pd.to_datetime(Pot_Storms['Date'],dayfirst = True, format='mixed')\n",
    "    Pot_Storms['Time Between Potential Storms'] = Pot_Storms['Date'].diff().dt.days\n",
    "    # Identify new groups where the difference is not 1 day\n",
    "    Pot_Storms['Storms'] = (Pot_Storms['Time Between Potential Storms'] != 1).cumsum()\n",
    "    \n",
    "    #Now add next periods pressure\n",
    "    Pot_Storms.set_index('Date',inplace = True)\n",
    "    Pot_Storms['MSLP Next Time'] = Pot_Storms['MSLP'] + Pot_Storms['Tendency']\n",
    "    Wind_Storms_Included = Pot_Storms\n",
    "    if Raindays_Av == True:\n",
    "        #filter raindays into it\n",
    "        Pot_Storms = pd.merge(Pot_Storms, Raindays, how='inner', left_index=True, right_index=True)\n",
    "        Pot_Storms = pd.merge(Pot_Storms, Rainfall, how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "        #Identify if rainfall and raindays is in there\n",
    "        Pot_Storms = Pot_Storms[Pot_Storms[Raindays.columns[0]] == 1]\n",
    "        \n",
    "        #Find the time between storms again\n",
    "        Pot_Storms['Time Between Potential Storms'] = Pot_Storms.index.diff().days\n",
    "\n",
    "        #Indentify winter storms\n",
    "        Pot_Storms = Pot_Storms[(Pot_Storms.index.month == 6) | (Pot_Storms.index.month == 7) | (Pot_Storms.index.month == 8)]\n",
    "\n",
    "        # Identify new groups where the difference is not 1 day\n",
    "        Pot_Storms['Storms'] = (Pot_Storms['Time Between Potential Storms'] != 1).cumsum()\n",
    "\n",
    "\n",
    "\n",
    "    if Extreme == True:\n",
    "        #Identify storms with the most extreme tendencys 0.01 and 0.99\n",
    "        E_percentiles_ind_6hr = T_6hr.dropna().groupby('month')['Tendency'].quantile([0.001, 0.999])\n",
    "        E_percentiles_ind_18hr = T_18hr.dropna().groupby('month')['Tendency'].quantile([0.001, 0.999])\n",
    "        # Convert to DataFrame and unstack the quantile level\n",
    "        E_percentiles_ind_6hr = E_percentiles_ind_6hr.unstack(level=1)\n",
    "        E_percentiles_ind_18hr = E_percentiles_ind_18hr.unstack(level=1)\n",
    "        # rename columns for clarity\n",
    "        E_percentiles_ind_6hr.columns = ['0.1th', '99.9th']\n",
    "        E_percentiles_ind_18hr.columns = ['0.1th', '99.9th']\n",
    "\n",
    "        #To find some of the biggest pressure jumps without rainfall included\n",
    "        extreme_storms_BF_RF = []       \n",
    "        Pot_Storms_BF_RF = Pot_Storms.loc[:'1879'].copy().reset_index()\n",
    "        for i in range(len(Pot_Storms_BF_RF)):\n",
    "            Data = Pot_Storms_BF_RF.iloc[i]\n",
    "            Date = Data['Date']\n",
    "            Month = Date.month\n",
    "            Tend = Data['Tendency']\n",
    "            Tendency_Window = Data['Hours_to_next']\n",
    "            if (Tendency_Window == 6):\n",
    "               if (Tend <= E_percentiles_ind_6hr['0.1th'][Month]) | (Tend >= E_percentiles_ind_6hr['99.9th'][Month]):\n",
    "                   extreme_storms_BF_RF.append(Data)\n",
    "               else:\n",
    "                   pass\n",
    "            elif (Tendency_Window == 18):\n",
    "                if (Tend <= E_percentiles_ind_18hr['0.1th'][Month]) | (Tend >= E_percentiles_ind_18hr['99.9th'][Month]):\n",
    "                    extreme_storms_BF_RF.append(Data)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        extreme_storms_BF_RF = pd.DataFrame(extreme_storms_BF_RF) #in the 1800s\n",
    "\n",
    "        extreme_storms_AF_RF =Pot_Storms[Pot_Storms[P_RF.columns[0]] >= 30].loc['1880':]\n",
    "    \n",
    "        extreme_storms = pd.concat([extreme_storms_BF_RF.set_index('Date'),extreme_storms_AF_RF], axis = 0).sort_index()\n",
    "        extreme_storms['Time Between Potential Storms'] = extreme_storms.index.diff().days\n",
    "        extreme_storms['Storms'] = (extreme_storms['Time Between Potential Storms'] != 1).cumsum()\n",
    "\n",
    "        es_cols = extreme_storms.columns\n",
    "    \n",
    "        extreme_storms =extreme_storms.rename(columns = {es_cols[5] :'Next MSLP Reading (hPa)'})\n",
    "        extreme_storms =extreme_storms.rename(columns = {es_cols[1] :'Hours To Next MSLP Reading'})\n",
    "        extreme_storms =extreme_storms.rename(columns = {P_MSLP_SD.columns[0] :'MSLP (hPa)'})\n",
    "        extreme_storms =extreme_storms.rename(columns = {es_cols[2] :'MSLP Change (hPa)'})\n",
    "        extreme_storms =extreme_storms.rename(columns = {es_cols[7] :'Precipitation (mm)'})\n",
    "        extreme_storms =extreme_storms.rename(columns = {es_cols[4] :'Storm ID'})\n",
    "\n",
    "        del extreme_storms[es_cols[6]]\n",
    "        extreme_storms = extreme_storms.reset_index().set_index('Storm ID')\n",
    "        extreme_storms = extreme_storms[['Date','MSLP (hPa)', 'Precipitation (mm)', 'Hours To Next MSLP Reading', 'Next MSLP Reading (hPa)', 'MSLP Change (hPa)']] \n",
    "        extreme_storms['Hours To Next MSLP Reading'] =extreme_storms['Hours To Next MSLP Reading'].round(0)\n",
    "        return(extreme_storms)\n",
    "\n",
    "    if RD == False:\n",
    "        Wind_Storms_Included = Wind_Storms_Included[(Wind_Storms_Included.index.month == 6) | (Wind_Storms_Included.index.month == 7) | (Wind_Storms_Included.index.month == 8)]\n",
    "        # Identify new groups where the difference is not 1 day\n",
    "        Wind_Storms_Included['Time Between Potential Storms'] = Wind_Storms_Included.index.diff().days\n",
    "        Wind_Storms_Included['Storms'] = (Wind_Storms_Included['Time Between Potential Storms'] != 1).cumsum()\n",
    "        return(Wind_Storms_Included)\n",
    "    else:\n",
    "        return(Pot_Storms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b2957-2204-4787-85fa-2e90af3cbf94",
   "metadata": {},
   "source": [
    "# STORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5e0c8e-c43e-44e9-afc4-c900ef51c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STORM_MSLP_ONLY(Events_Vec, Dataset,Titles):\n",
    "    #Lets play with the predata\n",
    "    # Shape of Australia\n",
    "    borders_gdf = gpd.read_file(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\FINALISED\\Shape Files\\World Shape\\ne_10m_admin_0_countries.shp\")\n",
    "    \n",
    "    # If needed, set the crs (coordinate reference system) for the GeoDataFrame\n",
    "    # For example, if your data is in WGS84 (EPSG:4326), you can set the crs as follows:\n",
    "    borders_gdf.crs = 'EPSG:4326'\n",
    "\n",
    "    dot = 4\n",
    "    #Find the length of the events\n",
    "    length_events = len(Events_Vec)\n",
    "    #Lets create a subplot\n",
    "    fig, ax = plt.subplots(nrows=length_events, ncols=4, figsize=(21,3.2*length_events))\n",
    "    \n",
    "    #Lets make sure our borders are done correctly\n",
    "    lat_min, lat_max = Dataset['lat'].min().item(), Dataset['lat'].max().item()\n",
    "    lon_min, lon_max = Dataset['lon'].min().item(), Dataset['lon'].max().item()\n",
    "\n",
    "    #Now lets create the anomaly for overall\n",
    "    #Mean_Pressure_81_10 = Dataset.sel(time=slice('1981', '2010')).mean(dim='time')\n",
    "    \n",
    "    #For Winter Storms\n",
    "    Mean_Pressure_81_10 = Dataset.sel(time=slice('1961', '1990'))\n",
    "    Mean_Pressure_81_10 = Mean_Pressure_81_10.sel(time=Mean_Pressure_81_10['time'].dt.month.isin([6, 7, 8])).mean(dim='time')\n",
    "\n",
    "    Pressure_Anomaly = Dataset - Mean_Pressure_81_10  \n",
    "    \n",
    "    #Colours \n",
    "    colorbar = 'RdBu_r'\n",
    "    \n",
    "    #Range\n",
    "    P_range = np.arange(-16, 16 + 2, 2)\n",
    "\n",
    "    \n",
    "    #Now we have to create the for loop that produces the figure\n",
    "    for i, event_date in enumerate(Events_Vec):\n",
    "        #Day Of\n",
    "        DOE = datetime.strptime(event_date, '%Y-%m-%d')\n",
    "        \n",
    "        #Day Before\n",
    "        DBE = DOE - timedelta(days=1)\n",
    "\n",
    "        #Day After\n",
    "        DAE = DOE + timedelta(days=1)\n",
    "        \n",
    "        # Set the axes for the current row\n",
    "        row_axes = ax[i]\n",
    "        \n",
    "        # Plot for Day Before\n",
    "        P_cont = Pressure_Anomaly.prmsl.sel(time=DBE).plot.contourf(ax=row_axes[0], cmap=colorbar, levels=P_range, add_colorbar=False)\n",
    "        DBE_YMD = DBE.strftime('%Y-%m-%d')\n",
    "        row_axes[0].set_title('{}'.format(DBE_YMD),fontsize = 12)\n",
    "        # Plot for Day Of\n",
    "        P_cont = Pressure_Anomaly.prmsl.sel(time=DOE).plot.contourf(ax=row_axes[1], cmap=colorbar, levels=P_range, add_colorbar=False)\n",
    "        DOE_YMD = DOE.strftime('%Y-%m-%d')\n",
    "        row_axes[1].set_title('{}'.format(DOE_YMD),fontsize = 12)\n",
    "\n",
    "        # Plot for Day After\n",
    "        P_cont = Pressure_Anomaly.prmsl.sel(time=DAE).plot.contourf(ax=row_axes[2], cmap=colorbar, levels=P_range, add_colorbar=False)\n",
    "        DAE_YMD = DAE.strftime('%Y-%m-%d')\n",
    "        row_axes[2].set_title('{}'.format(DAE_YMD),fontsize = 12)\n",
    "\n",
    "\n",
    "\n",
    "        for j in [0,1,2]:\n",
    "            #Plot Australian Border\n",
    "            Border = borders_gdf.plot(ax=row_axes[j], alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "            #Set the plot borders\n",
    "            row_axes[j].set_xlim(lon_min, lon_max)\n",
    "            row_axes[j].set_ylim(lat_min, lat_max)\n",
    "            #Plot Perth\n",
    "            Perth = row_axes[j].plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "\n",
    "            row_axes[j].set_xlabel('')\n",
    "            row_axes[j].set_ylabel('')\n",
    "            # Remove tick labels on the x and y axes\n",
    "            row_axes[j].set_xticklabels([])\n",
    "            row_axes[j].set_yticklabels([])\n",
    "            row_axes[j].tick_params(axis='both', which='both', bottom=False, left=False)\n",
    "\n",
    "        ax[i,0].set_ylabel('{}'.format(Titles[i]),fontsize = 12)\n",
    "\n",
    "    #Fix the final column\n",
    "    ax15 = ax[:,3]\n",
    "    cbar_PM = fig.colorbar(P_cont, ax=ax15, location='left', shrink=0.5)\n",
    "    # Set the font size of colorbar ticks\n",
    "    cbar_PM.ax.tick_params(labelsize=14)\n",
    "    # Create a text object for the label\n",
    "    cbar_label = fig.text(0.77, 0.5, 'Mean Sea Level\\nPressure Anomaly (hPa)', rotation=90, va='center', fontsize=18)\n",
    "\n",
    "    for i in range(length_events):\n",
    "        # Remove the last subplot in the rows\n",
    "        fig.delaxes(ax[i,3])\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "#fig, ax = STORM__MSLP_ONLY(['1951-07-16', '1866-07-28','1972-02-10','1907-06-20','1915-01-25'], Daily_Pressure,['Storm','Storm','Tropical\\nCyclone','Storm','Tropical\\nCyclone'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c68c94-0a87-44ac-8ed5-3a0500d6de8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a85b0b-186a-40d7-8886-ebafe66d8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def STORM_V2(Year, date, hour ,date_for_xlim, PresTS, TempTS, Type):\n",
    "    #Lets play with the predata\n",
    "    # Shape of Australia\n",
    "    borders_gdf = gpd.read_file(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\FINALISED\\Shape Files\\World Shape\\ne_10m_admin_0_countries.shp\")\n",
    "    \n",
    "    # If needed, set the crs (coordinate reference system) for the GeoDataFrame\n",
    "    # For example, if your data is in WGS84 (EPSG:4326), you can set the crs as follows:\n",
    "    borders_gdf.crs = 'EPSG:4326'\n",
    "\n",
    "    #20CR Pressure\n",
    "    T2M =  xr.open_dataset(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\DATA\\Data\\20CR\\FOR EXTREME EVENT EXAMPLES\\air.2m.{}.nc\".format(Year))\n",
    "    APCP =  xr.open_dataset(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\DATA\\Data\\20CR\\FOR EXTREME EVENT EXAMPLES\\apcp.{}.nc\".format(Year))\n",
    "    MSLP =  xr.open_dataset(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\DATA\\Data\\20CR\\FOR EXTREME EVENT EXAMPLES\\prmsl.{}.nc\".format(Year))\n",
    "    UWIND =  xr.open_dataset(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\DATA\\Data\\20CR\\FOR EXTREME EVENT EXAMPLES\\uwnd.10m.{}.nc\".format(Year))\n",
    "    VWIND =  xr.open_dataset(r\"C:\\Users\\jarra\\Desktop\\RA ASSISTANT\\DATA\\Data\\20CR\\FOR EXTREME EVENT EXAMPLES\\vwnd.10m.{}.nc\".format(Year))\n",
    "\n",
    "    #Add 8hour to UTC to get it into AWST\n",
    "    T2M = T2M.assign_coords(time=T2M.time + np.timedelta64(8, 'h'))\n",
    "    APCP = APCP.assign_coords(time=APCP.time + np.timedelta64(8, 'h'))\n",
    "    MSLP = MSLP.assign_coords(time=MSLP.time + np.timedelta64(8, 'h'))\n",
    "    UWIND = UWIND.assign_coords(time=UWIND.time + np.timedelta64(8, 'h'))\n",
    "    VWIND = VWIND.assign_coords(time=VWIND.time + np.timedelta64(8, 'h'))\n",
    "\n",
    "    #Convert to degC\n",
    "    T2M = T2M.air -273.15\n",
    "\n",
    "    #Convert to hPa\n",
    "    MSLP = MSLP.prmsl*0.01\n",
    "\n",
    "    #Back to dataset\n",
    "    MSLP= MSLP.to_dataset()\n",
    "    T2M = T2M.to_dataset()\n",
    "    APCP = APCP\n",
    "    ################DATES\n",
    "    \n",
    "    \n",
    "    #Spatial Series timeline of event\n",
    "    Storm_1851_P20CR = MSLP.sel(time=slice(date_for_xlim[0], date_for_xlim[1]))\n",
    "    Storm_1851_T20CR = T2M.sel(time=slice(date_for_xlim[0], date_for_xlim[1]))\n",
    "    Storm_1851_Uwnd20CR = UWIND.sel(time=slice(date_for_xlim[0], date_for_xlim[1]))\n",
    "    Storm_1851_Vwnd20CR = VWIND.sel(time=slice(date_for_xlim[0], date_for_xlim[1]))\n",
    "    Storm_1851_APCP20CR = APCP.sel(time=slice(date_for_xlim[0], date_for_xlim[1]))\n",
    "\n",
    "\n",
    "    #COMBINE U AND V WIND\n",
    "    Storm_1851_Wnd20CR = np.sqrt(Storm_1851_Uwnd20CR.uwnd**2 + Storm_1851_Vwnd20CR.vwnd**2)\n",
    "    Storm_1851_Wnd20CR = Storm_1851_Wnd20CR.to_dataset(name='wnd_spd')\n",
    "\n",
    "    P20CR = Storm_1851_P20CR\n",
    "    T20CR =  Storm_1851_T20CR\n",
    "    WND20CR = Storm_1851_Wnd20CR\n",
    "    APCP20CR = Storm_1851_APCP20CR\n",
    "\n",
    "  \n",
    "    TempTS =  TempTS.loc[date_for_xlim[0]: date_for_xlim[1]]\n",
    "    PresTS =  PresTS.loc[date_for_xlim[0]: date_for_xlim[1]]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################\n",
    "    '''\n",
    "    VARIABLES\n",
    "    '''\n",
    "    ###FOR LABELS###\n",
    "    #Fontsize for title\n",
    "    TL = 26\n",
    "    #Fontsize for x and y \n",
    "    XYL = 16\n",
    "    #Size of Perth\n",
    "    dot = 8\n",
    "    \n",
    "    \n",
    "    # Convert date strings to datetime objects\n",
    "    date_for_xlim = [datetime.strptime(date_str, '%Y-%m-%d') for date_str in date_for_xlim]\n",
    "    \n",
    "    #Date of Impact \n",
    "    Event = date\n",
    "    # Date of Event (DOE)\n",
    "    DOE = datetime.strptime(date, '%Y-%m-%d')\n",
    "    # Adjust the hour of the event\n",
    "    DOE = DOE.replace(hour=hour)\n",
    "    \n",
    "    # Calculate the day before and after the event\n",
    "    DBE = DOE - timedelta(hours=3)\n",
    "    DAE = DOE + timedelta(hours=3)\n",
    "    \n",
    "    #Lat and Lon Coords\n",
    "    lat_min, lat_max = -43, -25\n",
    "    lon_min, lon_max = 100,  130\n",
    "\n",
    "    #Colours \n",
    "    TM_C = 'coolwarm'\n",
    "    PM_C = 'PRGn'\n",
    "    RM_C = 'Blues'\n",
    "    WM_C = 'Reds'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Get the datasetts to the respective region\n",
    "    # Define the latitude and longitude bounds\n",
    "    lon_slice = slice(lon_min, lon_max)\n",
    "    lat_slice = slice(lat_min, lat_max)\n",
    "\n",
    "    # Select the desired geographical area from the dataset\n",
    "    P_subset = P20CR.sel(lon=lon_slice, lat=lat_slice)    \n",
    "    T_subset = T20CR.sel(lon=lon_slice, lat=lat_slice)    \n",
    "    R_subset = APCP20CR.sel(lon=lon_slice, lat=lat_slice)    \n",
    "    W_subset = WND20CR.sel(lon=lon_slice, lat=lat_slice)    \n",
    "    \n",
    "    #Ranges of Data, so Spread, and mean\n",
    "    PMean = [np.floor(P_subset.prmsl.min().values.item()),np.ceil(P_subset.prmsl.max().values.item())]\n",
    "    TMean = [np.floor(T_subset.air.min().values.item()),np.ceil(T_subset.air.max().values.item())]\n",
    "    RMean = [np.floor(R_subset.apcp.min().values.item()),np.ceil(R_subset.apcp.max().values.item())]\n",
    "    WMean = [np.floor(W_subset.wnd_spd.min().values.item()),np.ceil(W_subset.wnd_spd.max().values.item())]\n",
    "    # Set contour levels as integer values\n",
    "    TM_levels = np.arange(TMean[0], TMean[1] + 1,1)\n",
    "    PM_levels = np.arange(PMean[0], PMean[1] + 1,1)\n",
    "    RM_levels = np.arange(0, RMean[1] + 1,0.2)\n",
    "    WM_levels = np.arange(0, WMean[1] + 1,1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Start the plot\n",
    "    fig = plt.figure(figsize=(26, 24))\n",
    "    #gs = fig.add_gridspec(5, 4)\n",
    "    gs = fig.add_gridspec(5, 4, height_ratios=[1, 1.5, 1, 1, 1])\n",
    "\n",
    "    ##############################################################\n",
    "    '''\n",
    "    TIMESERIES (TEMPERATURE AND PRESSURE)\n",
    "    '''\n",
    "\n",
    "    # Create a twinax plot for Temp and Pressure from the timeseries\n",
    "    \n",
    "    # Plot Pressure\n",
    "    ax1 = fig.add_subplot(gs[0, 0:3])\n",
    "    ax1.plot(PresTS, color='red', marker='x', linestyle='-', markerfacecolor='red')\n",
    "\n",
    "    # Temperature\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(TempTS, color='blue', marker='o', linestyle='--', markerfacecolor='blue')\n",
    "\n",
    "    # Set x-axis ticks and labels using matplotlib.dates\n",
    "    locator = mdates.DayLocator(interval=1)  # Interval of 2 days\n",
    "\n",
    "    ax1.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))  # Adjust date format as needed\n",
    "    # Show all vertical grid lines\n",
    "    ax1.xaxis.grid(which='both', linestyle='-', linewidth=0.5)\n",
    "    # Set x-axis label and adjust fontsize\n",
    "    ax1.yaxis.grid(which='both', linestyle='-', linewidth=0.5)\n",
    "    # Adjust fontsize of x-axis tick labels\n",
    "    ax1.tick_params(axis='x', labelsize=12, rotation=45)  # Adjust labelsize as needed\n",
    "    ax1.tick_params(axis='y',labelsize=12)\n",
    "    \n",
    "    # Set y-axis tick colors to match the lines\n",
    "    ax1.tick_params(axis='y', colors='red')\n",
    "    ax2.tick_params(axis='y', colors='blue')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################################\n",
    "    '''\n",
    "    SPATIAL PLOT (PRESSURE)\n",
    "    '''\n",
    "    #PRESSURE\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "    ax = ax3\n",
    "    #Plot by contours\n",
    "    PM_contour = P_subset.prmsl.sel(time=DBE).plot.contourf(ax=ax, cmap=PM_C, levels=PM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "    \n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #PRESSURE\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "    ax = ax4\n",
    "    #Plot by contours\n",
    "    P_subset.prmsl.sel(time=DOE).plot.contourf(ax=ax, cmap=PM_C, levels=PM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #PRESSURE\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "\n",
    "    ax = ax5\n",
    "    #Plot by contours\n",
    "    P_subset.prmsl.sel(time=DAE).plot.contourf(ax=ax, cmap=PM_C, levels=PM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    ##############################################################\n",
    "    '''\n",
    "    SPATIAL PLOT (TEMPERATURE)\n",
    "    '''\n",
    "    #TEMPERATURE\n",
    "    ax6 = fig.add_subplot(gs[2, 0])\n",
    "\n",
    "    ax = ax6\n",
    "    #Plot by contours\n",
    "    TM_contour = T_subset.air.sel(time=DBE).plot.contourf(ax=ax, cmap=TM_C, levels=TM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #TEMPERATURE\n",
    "    ax7 = fig.add_subplot(gs[2, 1])\n",
    "\n",
    "    ax = ax7\n",
    "    #Plot by contours\n",
    "    T_subset.air.sel(time=DOE).plot.contourf(ax=ax, cmap=TM_C, levels=TM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #TEMPERATURE\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "\n",
    "    ax = ax8\n",
    "    #Plot by contours\n",
    "    T_subset.air.sel(time=DAE).plot.contourf(ax=ax, cmap=TM_C, levels=TM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    ##############################################################\n",
    "    '''\n",
    "    SPATIAL PLOT (RAINFALL)\n",
    "    '''\n",
    "    \n",
    "    #Rainfall\n",
    "    ax9 = fig.add_subplot(gs[3, 0])\n",
    "\n",
    "    ax = ax9\n",
    "    #Plot by contours\n",
    "    RM_contour = R_subset.apcp.sel(time=DBE).plot.contourf(ax=ax, cmap=RM_C, levels=RM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #Rainfall\n",
    "    ax10 = fig.add_subplot(gs[3, 1])\n",
    "\n",
    "    ax = ax10\n",
    "    #Plot by contours\n",
    "    R_subset.apcp.sel(time=DOE).plot.contourf(ax=ax, cmap=RM_C, levels=RM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #Rainfall\n",
    "    ax11 = fig.add_subplot(gs[3, 2])\n",
    "\n",
    "    ax = ax11\n",
    "    #Plot by contours\n",
    "    R_subset.apcp.sel(time=DAE).plot.contourf(ax=ax, cmap=RM_C, levels=RM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    \n",
    "    ##############################################################\n",
    "    '''\n",
    "    SPATIAL PLOT (WINDS)\n",
    "    '''\n",
    "    #WIND\n",
    "    ax12 = fig.add_subplot(gs[4, 0])\n",
    "\n",
    "    ax = ax12\n",
    "    #Plot by contours\n",
    "    WM_contour = W_subset.wnd_spd.sel(time=DBE).plot.contourf(ax=ax, cmap=WM_C, levels=WM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #WIND\n",
    "    ax13 = fig.add_subplot(gs[4, 1])\n",
    "\n",
    "    ax = ax13\n",
    "    #Plot by contours\n",
    "    W_subset.wnd_spd.sel(time=DOE).plot.contourf(ax=ax, cmap=WM_C, levels=WM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    #WIND\n",
    "    ax14 = fig.add_subplot(gs[4, 2])\n",
    "\n",
    "    ax = ax14\n",
    "    #Plot by contours\n",
    "    W_subset.wnd_spd.sel(time=DAE).plot.contourf(ax=ax, cmap=WM_C, levels=WM_levels, add_colorbar=False)\n",
    "\n",
    "    #Add the Australian Border\n",
    "    Border = borders_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', facecolor='none', linewidth=1)\n",
    "\n",
    "    #Set the plot borders\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)\n",
    "\n",
    "    #Add Perth\n",
    "    Perth = ax.plot(115.8605, -31.9505, 'ko', markersize=dot, label='Perth')\n",
    "    ax.set_title('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    \n",
    "    \n",
    "    ax15 = fig.add_subplot(gs[1, 3])\n",
    "    cbar_PM = fig.colorbar(PM_contour, ax=ax15, location='left')\n",
    "    ax16 = fig.add_subplot(gs[2, 3])\n",
    "    cbar_TM = fig.colorbar(TM_contour, ax=ax16, location='left')\n",
    "    ax17 = fig.add_subplot(gs[3, 3])\n",
    "    cbar_RM = fig.colorbar(RM_contour, ax=ax17, location='left')\n",
    "    ax18 = fig.add_subplot(gs[4, 3])\n",
    "    cbar_WM = fig.colorbar(WM_contour, ax=ax18, location='left')\n",
    "    # Removing empty subplots\n",
    "    fig.delaxes(ax15)\n",
    "    fig.delaxes(ax16)\n",
    "    fig.delaxes(ax17)\n",
    "    fig.delaxes(ax18)\n",
    "    \n",
    "    \n",
    "    #tITLE ETC\n",
    "    ax1.set_title('Pressure and Temperature Timeseries',fontsize = TL)\n",
    "    ax1.set_xlabel('Date',fontsize = XYL)\n",
    "    ax1.set_ylabel('MSLP (hPa)',color = 'red',fontsize = XYL)\n",
    "    ax2.set_ylabel('Temperature ($^\\circ$C)',color = 'blue',fontsize = XYL)\n",
    "    \n",
    "    #tITLE ETC\n",
    "    ax4.set_title('20th Century Reanalysis Data',fontsize = TL)\n",
    "    ax3.set_ylabel('3-Hourly Mean Sea Level\\nPressure (hPa)',fontsize = XYL)\n",
    "    ax6.set_ylabel('3-Hourly Temperature\\nat 2m ($^\\circ$C)',fontsize = XYL)\n",
    "    ax9.set_ylabel('3-Hourly Accumulation\\nPrecipitation (mm)',fontsize = XYL)\n",
    "    ax12.set_ylabel('3-Hourly Wind\\nSpeed at 10m (m/s)',fontsize = XYL)\n",
    "    ax12.set_xlabel('{}-{}-{} {}:00'.format(DBE.day,DBE.month,DBE.year,DBE.hour),fontsize = XYL)\n",
    "    ax13.set_xlabel('{}-{}-{} {}:00'.format(DOE.day,DOE.month,DOE.year,DOE.hour),fontsize = XYL)\n",
    "    ax14.set_xlabel('{}-{}-{} {}:00'.format(DAE.day,DAE.month,DAE.year,DAE.hour),fontsize = XYL)\n",
    "    plt.suptitle('{} {}'.format(Year, Type), fontsize = 28, y = 0.92, x = 0.42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3d1a4-3f0b-4196-a919-96c1bde98c65",
   "metadata": {},
   "source": [
    "# word save table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2601c22-2f39-4275-bd69-2c1f386516c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save potential storms\n",
    "def save_to_word(data,output):\n",
    "    data = data.reset_index()\n",
    "    data.replace(np.nan, 'N/A', inplace=True)\n",
    "    \n",
    "    # Convert Date to string format\n",
    "    data['Date'] = data['Date'].dt.strftime('%d-%m-%Y')\n",
    "    \n",
    "    # Helper function to set font and center-align text\n",
    "    def set_cell_font(cell, font_name=\"Times New Roman\", font_size=10):\n",
    "        for paragraph in cell.paragraphs:\n",
    "            paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "            for run in paragraph.runs:\n",
    "                run.font.name = font_name\n",
    "                run.font.size = Pt(font_size)\n",
    "                # Ensure font is correctly set in XML\n",
    "                r = run._element\n",
    "                rPr = r.get_or_add_rPr()\n",
    "                rFonts = rPr.find(qn('w:rFonts'))\n",
    "                if rFonts is None:\n",
    "                    rFonts = OxmlElement('w:rFonts')\n",
    "                    rPr.append(rFonts)\n",
    "                rFonts.set(qn('w:ascii'), font_name)\n",
    "                rFonts.set(qn('w:hAnsi'), font_name)\n",
    "            # Create Word document\n",
    "    doc = Document()\n",
    "    doc.add_heading('Extreme Storms Table', level=1)\n",
    "    \n",
    "    # Create table\n",
    "    table = doc.add_table(rows=1, cols=len(data.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    \n",
    "    # Add headers\n",
    "    hdr_cells = table.rows[0].cells\n",
    "    for i, col in enumerate(data.columns):\n",
    "        hdr_cells[i].text = str(col)\n",
    "        set_cell_font(hdr_cells[i])\n",
    "    \n",
    "    # Add data rows\n",
    "    for _, row in data.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for i, item in enumerate(row):\n",
    "            if isinstance(item, (float, int)):\n",
    "                row_cells[i].text = str(round(item, 1))\n",
    "            else:\n",
    "                row_cells[i].text = str(item)\n",
    "            set_cell_font(row_cells[i])\n",
    "    \n",
    "    doc.save(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5967c85-5897-4922-b275-09ad468026a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD STORM FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee6e52d-f797-402b-907d-284f2c53b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Storm_Finder_AP_2009(Pressure,Months_Inc, Tendency = False):\n",
    "\n",
    "    '''\n",
    "    Pressure - Subdaily pressure record\n",
    "\n",
    "    Output \n",
    "    Potential Storms 1 to 99\n",
    "    Potential Pressure Extreme Storms 0.1 to 99.9\n",
    "    '''\n",
    "    #Fix data so its all filled up with 24hours\n",
    "    All_Dates = pd.date_range(start=Pressure.index.min(), end=Pressure.index.max(), freq='h')\n",
    "    All_Dates = pd.DataFrame(index=All_Dates, columns=[Pressure.columns[0]])\n",
    "    \n",
    "    #Insert the missing hours into the Pressure dataset\n",
    "    Pressure = All_Dates.combine_first(Pressure)\n",
    "    \n",
    "    #Extract 9am and 3pm only\n",
    "    Pressure = Pressure[Pressure.index.hour.isin([9, 15])]\n",
    "    \n",
    "    # Make sure Pressure is sorted by datetime index\n",
    "    Pressure = Pressure.sort_index()\n",
    "    \n",
    "    # Calculate time difference to the next time (in hours)\n",
    "    Pressure[\"Hours_to_next\"] = (Pressure.index.to_series().shift(-1) - Pressure.index.to_series()).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate tendency (difference between current and next MSLP)\n",
    "    Pressure[\"Tendency\"] = Pressure[P_MSLP_SD.columns[0]].shift(-1) - Pressure[Pressure.columns[0]]\n",
    "    #The tendency is calculated on the day of focos so row 1003.0 3.8, means in 6hr its 1006.8\n",
    "    #This is to indentify the change on that particular day\n",
    "    \n",
    "    #Now we need to work out the 1st and 99th percentile tendency values of both 6hr and 18hr\n",
    "    T_6hr = Pressure[Pressure['Hours_to_next'] == 6][['Tendency']]\n",
    "    T_6hr[\"Tendency\"] = pd.to_numeric(T_6hr[\"Tendency\"], errors='coerce')\n",
    "    \n",
    "    T_18hr = Pressure[Pressure['Hours_to_next'] == 18][['Tendency']]\n",
    "    T_18hr[\"Tendency\"] = pd.to_numeric(T_18hr[\"Tendency\"], errors='coerce')\n",
    "    \n",
    "    \n",
    "    #Then work out 1st and 99th percentile for each month\n",
    "    T_6hr['month'] = T_6hr.index.month\n",
    "    T_18hr['month'] = T_18hr.index.month\n",
    "    \n",
    "    #For each month, now we have the range where the most extreme cases occur\n",
    "    percentiles_ind_6hr = T_6hr.dropna().groupby('month')['Tendency'].quantile([0.01, 0.99])\n",
    "    percentiles_ind_18hr = T_18hr.dropna().groupby('month')['Tendency'].quantile([0.01, 0.99])\n",
    "    \n",
    "    # Convert to DataFrame and unstack the quantile level\n",
    "    percentiles_ind_6hr = percentiles_ind_6hr.unstack(level=1)\n",
    "    percentiles_ind_18hr = percentiles_ind_18hr.unstack(level=1)\n",
    "    \n",
    "    # rename columns for clarity\n",
    "    percentiles_ind_6hr.columns = ['1st', '99th']\n",
    "    percentiles_ind_18hr.columns = ['1st', '99th']\n",
    "\n",
    "    #6hr\n",
    "    date_kept_6 = []\n",
    "    \n",
    "    #Iterate through the entire timeseries\n",
    "    for row in T_6hr.itertuples():\n",
    "        # Extract the month and tendency from the row\n",
    "        mth = row.month\n",
    "        tend = row.Tendency\n",
    "    \n",
    "        #Extract the 99th and 1st percentile\n",
    "        if (tend <= percentiles_ind_6hr['1st'][mth]) | (tend >= percentiles_ind_6hr['99th'][mth]):\n",
    "            date_kept_6.append(row.Index)\n",
    "        else:\n",
    "            x = 0\n",
    "    \n",
    "    #6hr\n",
    "    date_kept_18 = []\n",
    "    \n",
    "    #Iterate through the entire timeseries\n",
    "    for row in T_18hr.itertuples():\n",
    "        # Extract the month and tendency from the row\n",
    "        mth = row.month\n",
    "        tend = row.Tendency\n",
    "    \n",
    "        #Extract the 99th and 1st percentile\n",
    "        if (tend <= percentiles_ind_18hr['1st'][mth]) | (tend >= percentiles_ind_18hr['99th'][mth]):\n",
    "            date_kept_18.append(row.Index)\n",
    "        else:\n",
    "            x = 0\n",
    "    #Keeps date and hour\n",
    "    date_kept_6 = pd.DataFrame(date_kept_6, columns=['Date']).set_index('Date')\n",
    "    date_kept_18 = pd.DataFrame(date_kept_18, columns=['Date']).set_index('Date')\n",
    "\n",
    "    #Combine the potential storms together\n",
    "    Pot_Storms = pd.concat([date_kept_6,date_kept_18],axis = 1).sort_index()\n",
    "    #Merge with the actual tendency and MSLP data\n",
    "    Pot_Storms = pd.merge(Pot_Storms,Pressure,how = 'inner',left_index = True, right_index= True)\n",
    "    \n",
    "    #Remove duplicated dates\n",
    "    Pot_Storms['Date'] = Pot_Storms.index.strftime('%Y-%m-%d')\n",
    "    #Pot_Storms = Pot_Storms.drop_duplicates(subset='Date', keep='first')\n",
    "\n",
    "    #ID the storms\n",
    "    # Calculate the difference between consecutive dates\n",
    "    Pot_Storms['Date'] = pd.to_datetime(Pot_Storms['Date'],dayfirst = True, format='mixed')\n",
    "    Pot_Storms['Time Between Potential Storms'] = Pot_Storms['Date'].diff().dt.days\n",
    "    # Identify new groups where the difference is not 1 day\n",
    "    Pot_Storms['Storms'] = (Pot_Storms['Time Between Potential Storms'] > 1).cumsum() + 1\n",
    "    \n",
    "    #Now add next periods pressure\n",
    "    Pot_Storms.set_index('Date',inplace = True)\n",
    "    Pot_Storms['MSLP Next Time'] = Pot_Storms['MSLP'] + Pot_Storms['Tendency']\n",
    "    Wind_Storms_Included = Pot_Storms #During winter\n",
    "    \n",
    "    Winter_Storms = ChatJR.filter_dataframe_by_months(Wind_Storms_Included, Months_Inc)\n",
    "    \n",
    "    # Calculate the difference between consecutive dates\n",
    "    Winter_Storms['Time Between Potential Storms'] = Winter_Storms.index.diff().days\n",
    "    # Identify new groups where the difference is not 1 day\n",
    "    Winter_Storms['Storms'] = (Winter_Storms['Time Between Potential Storms'] > 1).cumsum()+1\n",
    "    es_cols = Winter_Storms.columns\n",
    "    \n",
    "    Winter_Storms =Winter_Storms.rename(columns = {P_MSLP_SD.columns[0] :'MSLP (hPa)'})\n",
    "    Winter_Storms =Winter_Storms.rename(columns = {es_cols[1] :'Hours To Next MSLP Reading'})\n",
    "    Winter_Storms =Winter_Storms.rename(columns = {es_cols[2] :'MSLP Change (hPa)'})\n",
    "    Winter_Storms =Winter_Storms.rename(columns = {es_cols[3] :'Days between Storms'})\n",
    "    Winter_Storms =Winter_Storms.rename(columns = {es_cols[4] :'Storm ID'})\n",
    "    Winter_Storms =Winter_Storms.rename(columns = {es_cols[5] :'Next MSLP Reading (hPa)'})\n",
    "    Winter_Storms = Winter_Storms.reset_index().set_index('Storm ID')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #Now lets get extreme winter storms based on pressure\n",
    "    #Identify storms with the most extreme tendencys 0.01 and 0.99\n",
    "    E_percentiles_ind_6hr = T_6hr.dropna().groupby('month')['Tendency'].quantile([0.001, 0.999])\n",
    "    E_percentiles_ind_18hr = T_18hr.dropna().groupby('month')['Tendency'].quantile([0.001, 0.999])\n",
    "    # Convert to DataFrame and unstack the quantile level\n",
    "    E_percentiles_ind_6hr = E_percentiles_ind_6hr.unstack(level=1)\n",
    "    E_percentiles_ind_18hr = E_percentiles_ind_18hr.unstack(level=1)\n",
    "    # rename columns for clarity\n",
    "    E_percentiles_ind_6hr.columns = ['0.1th', '99.9th']\n",
    "    E_percentiles_ind_18hr.columns = ['0.1th', '99.9th']\n",
    "\n",
    "    #To find some of the biggest pressure jumps without rainfall included\n",
    "    extreme_storms_BF_RF = []       \n",
    "    Pot_Storms_BF_RF = Pot_Storms.copy().reset_index()\n",
    "    for i in range(len(Pot_Storms_BF_RF)):\n",
    "        Data = Pot_Storms_BF_RF.iloc[i]\n",
    "        Date = Data['Date']\n",
    "        Month = Date.month\n",
    "        Tend = Data['Tendency']\n",
    "        Tendency_Window = Data['Hours_to_next']\n",
    "        if (Tendency_Window == 6):\n",
    "           if (Tend <= E_percentiles_ind_6hr['0.1th'][Month]) | (Tend >= E_percentiles_ind_6hr['99.9th'][Month]):\n",
    "               extreme_storms_BF_RF.append(Data)\n",
    "           else:\n",
    "               pass\n",
    "        elif (Tendency_Window == 18):\n",
    "            if (Tend <= E_percentiles_ind_18hr['0.1th'][Month]) | (Tend >= E_percentiles_ind_18hr['99.9th'][Month]):\n",
    "                extreme_storms_BF_RF.append(Data)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    extreme_storms_BF_RF = pd.DataFrame(extreme_storms_BF_RF) #in the 1800s\n",
    "    extreme_storms_BF_RF = extreme_storms_BF_RF.set_index('Date')\n",
    "    extreme_storms_BF_RF.index= pd.to_datetime(extreme_storms_BF_RF.index,format='mixed',dayfirst = True)\n",
    "    Extreme_Winter_Storms = ChatJR.filter_dataframe_by_months(extreme_storms_BF_RF,Months_Inc)\n",
    "\n",
    "\n",
    "    Extreme_Winter_Storms['Time Between Potential Storms'] = Extreme_Winter_Storms.index.diff().days\n",
    "    Extreme_Winter_Storms['Storms'] = (Extreme_Winter_Storms['Time Between Potential Storms'] > 1).cumsum()+1\n",
    "\n",
    "    es_cols = Extreme_Winter_Storms.columns\n",
    "\n",
    "    Extreme_Winter_Storms =Extreme_Winter_Storms.rename(columns = {P_MSLP_SD.columns[0] :'MSLP (hPa)'})\n",
    "    Extreme_Winter_Storms =Extreme_Winter_Storms.rename(columns = {es_cols[1] :'Hours To Next MSLP Reading'})\n",
    "    Extreme_Winter_Storms =Extreme_Winter_Storms.rename(columns = {es_cols[2] :'MSLP Change (hPa)'})\n",
    "    Extreme_Winter_Storms =Extreme_Winter_Storms.rename(columns = {es_cols[3] :'Days between Storms'})\n",
    "    Extreme_Winter_Storms =Extreme_Winter_Storms.rename(columns = {es_cols[4] :'Storm ID'})\n",
    "    Extreme_Winter_Storms =Extreme_Winter_Storms.rename(columns = {es_cols[5] :'Next MSLP Reading (hPa)'})\n",
    "    Extreme_Winter_Storms = Extreme_Winter_Storms.reset_index().set_index('Storm ID')\n",
    "    \n",
    "    Extreme_Winter_Storms = Extreme_Winter_Storms[['Date','MSLP (hPa)', 'Hours To Next MSLP Reading', 'Next MSLP Reading (hPa)', 'MSLP Change (hPa)']] \n",
    "    Extreme_Winter_Storms['Hours To Next MSLP Reading'] =Extreme_Winter_Storms['Hours To Next MSLP Reading'].round(0)\n",
    "    if (Tendency == True):\n",
    "        return(Winter_Storms,Extreme_Winter_Storms,[percentiles_ind_6hr,percentiles_ind_18hr, E_percentiles_ind_6hr,E_percentiles_ind_18hr])\n",
    "    else:\n",
    "        return(Winter_Storms,Extreme_Winter_Storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6528595-8fff-4048-9f9f-4fd9a817b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Storm_Finder_With_Rain(Storms_Before, Extreme_Storms_Before, Raindays, Rainfall, Rainfal_Target):\n",
    "\n",
    "    Storms =Storms_Before\n",
    "    Extreme_Storms = Extreme_Storms_Before\n",
    "\n",
    "    Storms = Storms.set_index('Date')\n",
    "    Extreme_Storms = Extreme_Storms.set_index('Date')\n",
    "    #filter raindays into it\n",
    "    Storms = pd.merge(Storms, Raindays, how='inner', left_index=True, right_index=True)\n",
    "    Storms = pd.merge(Storms, Rainfall, how='inner', left_index=True, right_index=True)\n",
    "    Storms.index.name = 'Date'\n",
    "\n",
    "    #filter raindays into it\n",
    "    Extreme_Storms = pd.merge(Extreme_Storms, Raindays, how='inner', left_index=True, right_index=True)\n",
    "    Extreme_Storms = pd.merge(Extreme_Storms, Rainfall, how='inner', left_index=True, right_index=True)\n",
    "    Extreme_Storms.index.name = 'Date'\n",
    "\n",
    "    #Identify if rainfall and raindays is in there\n",
    "    Storms = Storms[Storms[Raindays.columns[0]] == 1]\n",
    "    Extreme_Storms = Extreme_Storms[Extreme_Storms[Raindays.columns[0]] == 1]\n",
    "    \n",
    "    #Find the time between storms again\n",
    "    Storms['Days between Storms'] = Storms.index.diff().days\n",
    "    Extreme_Storms['Days between Storms'] = Extreme_Storms.index.diff().days\n",
    "    \n",
    "    # Identify new groups where the difference is not 1 day\n",
    "    Storms['Storm ID'] = (Storms['Days between Storms'] > 1).cumsum()+1\n",
    "    Extreme_Storms_RF =Storms[Storms['RAINFALL'] >=Rainfal_Target]\n",
    "\n",
    "    #Extreme Storms on a Rainfall basis\n",
    "    Extreme_Storms = Extreme_Storms.combine_first(Extreme_Storms_RF)\n",
    "   \n",
    "    Extreme_Storms['Days between Storms'] = Extreme_Storms.index.diff().days\n",
    "    Extreme_Storms['Storm ID'] = (Extreme_Storms['Days between Storms'] > 1).cumsum()+1\n",
    "\n",
    "\n",
    "    Storms =  Storms.reset_index().set_index('Storm ID')\n",
    "    Extreme_Storms =  Extreme_Storms.reset_index().set_index('Storm ID')\n",
    "    Extreme_Storms = Extreme_Storms[['Date','MSLP (hPa)','RAINFALL', 'Hours To Next MSLP Reading','Next MSLP Reading (hPa)','MSLP Change (hPa)']]\n",
    "    Extreme_Storms = Extreme_Storms.rename(columns={\"RAINFALL\": \"Rainfall\"})\n",
    "    Storms = Storms.rename(columns={\"RAINFALL\": \"Rainfall\"})\n",
    "    \n",
    "    return(Storms,Extreme_Storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3b29c-d3ec-470e-978b-8b0cdff656a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7eac2b-d91c-43ee-80e8-ba9714928a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cea9e-e9de-49bb-811f-9fcda6f7d5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf5247-cd99-4d37-9ff8-baad8dd126d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581aeca4-e06b-441a-84ac-ae2b268c9405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e301658f-0278-4092-8e8e-0c9152b84f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa36a4-58a5-4528-adc2-e5c87c973a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17005b58-4be8-48eb-8083-cd8e9e98aa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708467c1-5636-40c3-b544-03f89f0a1fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8edf2b4-859d-46d4-acbd-24046133da32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ce0bb-3508-4502-acda-d3048123d226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
